[
["index.html", "Regression Models for Count Data: beyond the Poisson model Preface", " Regression Models for Count Data: beyond the Poisson model Wagner Hugo Bonat Walmes Marques Zeviani Eduardo Elias Ribeiro Jr Preface The main goal of this material is to provide a technical support for the students attending the course ’’Regression models for count data: beyond the Poisson model“, given as part of the XV Brazilian School of Regression models - March/2017 in Goiânia, Goiás, Brazil. The main goal of this course is to present a wide range of statistical models to deal with count data. We focus on parametric and second-moment specified models. We shall present the model specification along with strategies for model fitting and associated R(R Core Team 2015) code. Furthermore, this book-course and supplementary materials as R code and data sets are available for the students on the web page http://cursos.leg.ufpr.br/rmcd. We intend to keep the course in a level suitable for bachelor students who already attended a course on generalized linear models (Nelder and Wedderburn 1972). However, since the course also covers updated topics, it can be of interest of postgraduate students and researches in general. We designed the course for three hours of tuition. In the first part (two hours) of the course, we shall present the analysis of count data based on fully parametric models. After a brief introduction and motivation on count data, we present the Poisson, Gamma-Count, Poisson-Tweedie and COM-Poisson distributions. We explore their properties through a consideration of dispersion, zero-inflated and heavy tail indexes. Furthermore, the estimation and inference for these models based on the likelihood paradigm is discussed along with R code and worked examples. In the second part (one hour) of the course, we provide a brief introduction to the estimating function approach (Jørgensen and Knudsen 2004 ; Bonat and Jørgensen 2016) and discuss models based on second-moment assumptions in the style of Wedderburn (1974). In particular, we focus on the recently proposed Extended Poisson-Tweedie model (Bonat et al. 2016) and its special case the quasi-Poisson model. The estimating function approach adopted for estimation and inference is presented along with R code and data examples. The use of the R package mcglm (Bonat 2016) is discussed for fitting the extended Poisson-Tweedie model. We acknowledge our gratitude to the scientific committee of XV Brazilian regression model school for this opportunity. Department of Statistics, Paraná Federal University, Curitiba, PR, Brazil. March 27, 2017. "],
["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction The analysis of count data has received attention from the statistical community in the last four decades. Since the seminal paper published by Nelder and Wedderburn (1972), the class of generalized linear models (GLMs) have a proeminent role for regression modelling of normal and non-normal data. GLMs are fitted by a simple and efficient Newton score algorithm relying only on second-moment assumptions for estimation and inference. Furthermore, the theoretical background for GLMs is well established in the class of dispersion models (B. Jørgensen 1987; B. Jørgensen 1997) as a generalization of the exponential family of distributions. In spite of the flexibility of the GLM class, the Poisson distribution is the only choice for the analysis of count data in this framework. Thus, in practice there is probably an over-emphasis on the use of the Poisson distribution for count data. A well known limitation of the Poisson distribution is its mean and variance relationship, which implies that the variance equals the mean, referred to as equidispersion. In practice, however, count data can present other features, namely underdispersion (mean &gt; variance) and overdispersion (mean &lt; variance). There are many different possible causes for departures from the equidispersion. Furthermore, in practical data analysis a number of these could be involved. One possible cause of under/overdispersion is departure from the Poisson process. It is well known that the Poisson counts can be interpreted as the number of events in a given time interval where the arrival’s times are exponential distributed. When this assumption is violated the resulting counts can be under or overdispersed (W. M. Zeviani et al. 2014). Another possibility and probably more frequent cause of overdispersion is unobserved heterogeneity of experimental units. It can be due, for example, to correlation between individual responses, cluster sampling, ommitted covariates and others. In general, these departures from the Poisson distribution are manifested in the raw data as a zero-inflated or heavy-tailed count distribution. It is important to discuss the consequences of failing to take into account the under or overdispersion when analysing count data. In the case of overdispersion, the standard errors associated with the regression coefficients calculated under the Poisson assumption are too optimistic and associated hypothesis tests will tend to give false positive results by incorrectly rejecting null hypotheses. The opposite situation will appear in case of underdispersed data. In both cases, the Poisson model provides unreliable standard errors for the regression coefficients and hence potentially misleading inferences. However, the regression coefficients are still consistently estimated. The strategies for constructing alternative count distributions are related with the causes of the non-equidispersion. When departures from the Poisson process are plausible the class of duration dependence models (Winkelmann 2003) can be employed. This class of models changes the distribution of the time between events from the exponential to more general distributions, like gamma and inverse Gaussian. In this course, we shall discuss one example of this approach, namely, the Gamma-Count distribution (W. M. Zeviani et al. 2014). This distribution assumes that the time between events is gamma distributed, thus it can deal with under, equi and overdispersed count data. On the other hand, if unobserved heterogeneity is present its in general implies extra variability and consequently overdispersed count data. In this case, a Poisson mixtures is commonly applied. This approach consists of include random effects on the observation level, and thus take into account the unobserved heterogeneity. Probably, the most popular example of this approach is the negative binomial model, that corresponds to a Poisson-gamma mixtures. In this course, we shall present the Poisson-Tweedie family of distributions, which in turn corresponds to Poisson-Tweedie mixtures (Bonat et al. 2016; Jørgensen and Kokonendji 2015). Finally, a third approach to deal with non-equidispersed count data consists of generalize the Poisson distribution by adding an extra parameter to model under and overdispersion. Such a generalization can be done using the class of weighted Poisson distributions (Del Castillo and Pérez-Casany 1998). One popular example of this approach is the Conway–Maxwell–Poisson distribution (COM-Poisson) (K. F. Sellers and Shmueli 2010). The COM-Poisson is a member of the exponential family, has the Poisson and geometric distributions as special cases and the Bernoulli distribution as a limiting case. It can deal with both under and overdispersed count data. Thus, given the nice properties of the COM-Poisson distribution for handling count data, we choose to present this model as part of this short course. In this short course, we shall highlight and compare the ability of these distributions to deal with count data through a consideration of dispersion, zero-inflated and heavy tail indexes. Furthermore, we specify regression models and illustrate their application with two worked examples. In Chapter 2 we present the properties and regression models associated with the Poisson, Gamma-count, Poisson-Tweedie and COM-Poisson distributions. Furthermore, we compare these distributions using the dispersion, zero-inflated and heavy-tail indexes. Estimation and inference for these models based on the likelihood paradigm are discussed in Chapter 3. In Chapter 4, we extend the Poisson-Tweedie model using only second-moment assumptions and the fitting algorithm based on the estimating functions approach. Chapter \\(5\\) presents two worked examples. Finally, in Chapter \\(6\\) we discuss the general methods and propose some topics for future works. "],
["models.html", "Chapter 2 Count distributions: properties and regression models 2.1 Poisson distribution 2.2 Gamma-Count distribution 2.3 Poisson-Tweedie distribution 2.4 COM-Poisson distribution 2.5 Comparing count distributions", " Chapter 2 Count distributions: properties and regression models In this chapter, we present the probability mass function and discuss the main properties of the Poisson, Gamma-Count, Poisson-Tweedie and COM-Poisson distributions. 2.1 Poisson distribution The Poisson distribution is a notorious discrete distribution. It has a dual interpretation as a natural exponential family and as an exponential dispersion model. The Poisson distribution denoted by \\(P(\\mu)\\) has probability mass function \\[\\begin{eqnarray} p(y;\\mu) &amp;=&amp; \\frac{\\mu^y}{y!}\\exp\\{-\\mu\\} \\nonumber \\\\ &amp;=&amp; \\frac{1}{y!} \\exp \\{\\phi y - \\exp\\{\\phi\\} \\}, \\quad y \\in \\mathbb{N}_{0}, \\tag{2.1} \\end{eqnarray}\\] where \\(\\phi = \\log \\{\\mu\\} \\in \\mathbb{R}\\). Hence the Poisson is a natural exponential family with cumulant generator \\(\\kappa(\\phi) = \\exp\\{\\phi\\}\\). We have \\(\\mathrm{E}(Y) = \\kappa^{\\prime}(\\phi) = \\exp\\{\\phi\\} = \\mu\\) and \\(\\mathrm{var}(Y) = \\kappa^{\\prime \\prime}(\\phi) = \\exp\\{\\phi\\} = \\mu\\). The probability mass function (2.1) can be evaluated in R through the dpois() function. In order to specify a regression model based on the Poisson distribution, we consider a cross-section dataset, \\((y_i, x_i)\\), \\(i = 1,\\ldots, n\\), where \\(y_i\\)’s are iid realizations of \\(Y_i\\) according to a Poisson distribution. The Poisson regression models is defined by \\[Y_i \\sim P(\\mu_i), \\quad \\text{with} \\quad \\mu_i = g^{-1}(\\boldsymbol{x_i}^{\\top} \\boldsymbol{\\beta}).\\] In this notation, \\(\\boldsymbol{x_i}\\) and \\(\\boldsymbol{\\beta}\\) are (\\(q \\times 1\\)) vectors of known covariates and unknown regression parameters, respectively. Moreover, \\(g\\) is a standard link function, for which we adopt the logarithm link function, but potentially any other suitable link function could be adopted. 2.2 Gamma-Count distribution The Poisson distribution as presented in (2.1) follows directly from the natural exponential family and thus fits in the generalized linear models (GLMs) framework. Alternatively, the Poisson distribution can be derived by assuming independent and exponentially distributed times between events (W. M. Zeviani et al. 2014). This derivation allows for a flexible framework to specify more general models to deal with under and overdispersed count data. As point out by Winkelmann (2003) the distributions of the arrival times determine the distribution of the number of events. Following Winkelmann (1995), let \\({\\tau_k, k \\in \\mathbb{N}}\\) denote a sequence of waiting times between the \\((k-1)\\)th and the \\(k\\)th events. Then, the arrival time of the \\(y\\)th event is given by \\(\\nu_y = \\sum_{k = 1}^{y} \\tau_k\\), for \\(y = 1, 2, \\ldots\\). Furthermore, denote \\(Y\\) the total number of events in the open interval between \\(0\\) and \\(T\\). For fixed \\(T\\), \\(Y\\) is a count variable. Indeed, from the definitions of \\(Y\\) and \\(\\nu_y\\) we have that \\(Y &lt; y\\) iff \\(\\nu_y \\ge T\\), which in turn implies \\(P(Y &lt; y) = P(\\nu_y \\ge T) = 1 - F_y(T)\\), where \\(F_y(T)\\) denotes the cumulative distribution function of \\(\\nu_y\\). Furthermore, \\[\\begin{eqnarray} P(Y = y) &amp;=&amp; P(Y &lt; y+1) - P(Y &lt; y) \\nonumber \\\\ &amp;=&amp; F_y(T) - F_{y+1}(T). \\tag{2.2} \\end{eqnarray}\\] Equation (2.2) provides the fundamental relation between the distribution of arrival times and the distribution of counts. Moreover, this type of specification allows to derive a rich class of models for count data by choosing a distribution for the arrival times. In this material, we shall explore the Gamma-Count distribution which is obtained by specifying the arrival times distribution as gamma distributed. Let \\(\\tau_k\\) be identically and independently gamma distributed, with density distribution (dropping the index \\(k\\)) given by \\[\\begin{equation} f(\\tau; \\alpha, \\gamma) = \\frac{\\gamma^{\\alpha}}{\\Gamma(\\alpha)} \\tau^{\\alpha-1} \\exp\\{-\\gamma \\tau\\}, \\quad \\alpha, \\gamma \\in \\mathbb{R}^{+}. \\end{equation}\\] In this parameterization \\(\\mathrm{E}(\\tau) = \\alpha/\\gamma\\) and \\(\\mathrm{var}(\\tau) = \\alpha/\\gamma^2\\). Thus, by applying the convolution formula for gamma distributions, it is easy to show that the distribution of \\(\\nu_y\\) is given by \\[\\begin{equation} f_y(\\nu; \\alpha, \\gamma) = \\frac{\\gamma^{y\\alpha}}{\\Gamma(y\\alpha)} \\nu^{y\\alpha-1} \\exp\\{-\\gamma \\nu\\}. \\end{equation}\\] To derive the new count distribution, we have to evaluate the cumulative distribution function, which after the change of variable \\(u = \\gamma \\alpha\\) can be written as \\[\\begin{equation} F_y(T) = \\frac{1}{\\Gamma(y\\alpha)} \\int_0^{\\gamma T} u^{n\\alpha -1} \\exp\\{-u\\} du, \\tag{2.3} \\end{equation}\\] where the integral is the incomplete gamma function. We denote the right side of (2.3) as \\(G(\\alpha y, \\gamma T)\\). Thus, the number of event occurrences during the time interval \\((0,T)\\) has the two-parameter distribution function \\[\\begin{equation} P(Y = y) = G(\\alpha y, \\gamma T) - G(\\alpha (y + 1), \\gamma T), \\tag{2.4} \\end{equation}\\] for \\(y = 0, 1, \\ldots\\), where \\(\\alpha, \\gamma \\in \\mathbb{R}^+\\). Winkelmann (1995) showed that for integer \\(\\alpha\\) the probability mass function defined in (2.4) is given by \\[\\begin{equation} P(Y = y) = \\exp^{\\{-\\gamma T\\}} \\sum_{i = 0}^{\\alpha -1} \\frac{(\\gamma T)^{\\alpha y + i}}{\\alpha y + i}!. \\end{equation}\\] For \\(\\alpha = 1\\), \\(f(\\tau)\\) is the exponential distribution and (2.4) clearly simplifies to the Poisson distribution. The following R function can be used to evaluate the probability mass function of the Gamma-Count distribution. dgc &lt;- function(y, gamma, alpha, log = FALSE) { p &lt;- pgamma(q = 1, shape = y * alpha, rate = alpha * gamma) - pgamma(q = 1, shape = (y + 1) * alpha, rate = alpha * gamma) if(log == TRUE) {p &lt;- log(p)} return(p) } Although, numerical evaluation of (2.4) can easily be done, the moments (mean and variance) cannot be obtained in closed form. Winkelmann (1995) showed for a random variable \\(Y \\sim GC(\\alpha, \\gamma)\\), where \\(GC(\\alpha, \\gamma)\\) denotes a Gamma-Count distribution with parameters \\(\\alpha\\) and \\(\\gamma\\), \\(\\mathrm{E}(Y) = \\sum_{i = 1}^\\infty G(\\alpha i, \\gamma T)\\). Furthermore, for increasing \\(T\\) it holds that \\[\\begin{equation} Y(T) \\overset{a}{\\sim} N\\left(\\frac{\\gamma T}{\\alpha}, \\frac{\\gamma T}{\\alpha^2} \\right), \\end{equation}\\] thus the limiting variance-mean ratio equals a constant \\(1/\\alpha\\). Consequently, the Gamma-Count distribution displays overdispersion for \\(0 &lt; \\alpha &lt; 1\\) and underdispersion for \\(\\alpha &gt; 1\\). Figure 2.1 presents the probability mass function for some Gamma-Count distributions. We fixed the parameter \\(\\gamma = 10\\) and fit the parameter \\(\\alpha\\) in order to have dispersion index (\\(\\mathrm{DI} = \\mathrm{var}(Y)/\\mathrm{E}(Y)\\)) equaling to \\(0.5\\), \\(2\\), \\(5\\) and \\(20\\). Figure 2.1: Gamma-Count probability mass function by values of the dispersion index (DI). The Gamma-Count regression model assumes that the period at risk \\((T)\\) is identical for all observations, thus \\(T\\) may be set to unity without loss of generality. In the Gamma-count regression model, the parameters depend on a vector of individual covariates \\(\\boldsymbol{x}_i\\). Thus, the Gamma-Count regression model is defined by \\[\\begin{equation} \\mathrm{E}(\\tau_i | \\boldsymbol{x}_i) = \\frac{\\alpha}{\\gamma} = g^{-1}(-\\boldsymbol{x_i}^\\top \\boldsymbol{\\beta}). \\end{equation}\\] Consequently, the regression model is for the waiting times and not directly for the counts. Note that, \\(\\mathrm{E}(N_i | \\boldsymbol{x}_i) = \\mathrm{E}(\\tau_i | \\boldsymbol{x}_i)^{-1}\\) iff \\(\\alpha = 1\\). Thus, \\(\\hat{\\boldsymbol{\\beta}}\\) should be interpreted accordingly. \\(-\\beta\\) measures the percentage change in the expected waiting time caused by a unit increase in \\(x_i\\). The model parameters can be estimated using the maximum likelihood method as we shall discuss in Chapter 3. 2.3 Poisson-Tweedie distribution The Poisson-Tweedie distribution (Bonat et al. 2016; Jørgensen and Kokonendji 2015; El-Shaarawi, Zhu, and Joe 2011) consists of include Tweedie distributed random effects on the observation level of Poisson random variables, and thus to take into account unobserved heterogeneity. The Poisson-Tweedie family is given by the following hierarchical specification \\[\\begin{eqnarray} Y|Z &amp;\\sim&amp; \\mathrm{Poisson}(Z) \\\\ Z &amp;\\sim&amp; \\mathrm{Tw}_p(\\mu, \\phi), \\nonumber \\tag{2.5} \\end{eqnarray}\\] where \\(\\mathrm{Tw}_p(\\mu, \\phi)\\) denotes a Tweedie distribution (B. Jørgensen 1987 ; B. Jørgensen 1997) with probability function given by \\[\\begin{equation} f_{Z}(z; \\mu, \\phi, p) = a(z,\\phi,p) \\exp\\{(z\\psi - k_p(\\psi))/\\phi\\}. \\tag{2.6} \\end{equation}\\] In this notation, \\(\\mu = k^{\\prime}_p(\\psi)\\) is the expectation, \\(\\phi &gt; 0\\) is the dispersion parameter, \\(\\psi\\) is the canonical parameter and \\(k_p(\\psi)\\) is the cumulant function. Furthermore, \\(\\mathrm{var}(Z) = \\phi V(\\mu)\\) where \\(V(\\mu) = k^{\\prime \\prime}_p(\\psi)\\) is the variance function. Tweedie densities are characterized by power variance functions of the form \\(V(\\mu) = \\mu^p\\), where \\(p \\in (-\\infty ,0] \\cup [1,\\infty)\\) is an index determining the distribution. The support of the distribution depends on the value of the power parameter. For \\(p \\geq 2\\), \\(1 &lt; p &lt; 2\\) and \\(p = 0\\) the support corresponds to the positive, non-negative and real values, respectively. In these cases \\(\\mu \\in \\Omega\\), where \\(\\Omega\\) is the convex support (i.e. the interior of the closed convex hull of the corresponding distribution support). Finally, for \\(p &lt; 0\\) the support corresponds to the real values, however the expectation \\(\\mu\\) is positive. Here, we required \\(p \\geq 1\\), to make \\(\\mathrm{Tw}_p(\\mu, \\phi)\\) non-negative. The function \\(a(z,\\phi, p)\\) cannot be written in a closed form apart of the special cases corresponding to the Gaussian (\\(p = 0\\)), Poisson (\\(\\phi = 1\\) and \\(p = 1\\)), non-central gamma (\\(p = 3/2\\)), gamma (\\(p = 2\\)) and inverse Gaussian (\\(p = 3\\)) distributions (B. Jørgensen 1997). The compound Poisson distribution is obtained when \\(1 &lt; p &lt; 2\\). This distribution is suitable to deal with non-negative data with probability mass at zero and highly right-skewed (Andersen and Bonat 2016). The Poisson-Tweedie is an overdispersed factorial dispersion model (Jørgensen and Kokonendji 2015) and its probability mass function for \\(p &gt; 1\\) is given by \\[\\begin{equation} f(y;\\mu,\\phi,p) = \\int_0^\\infty \\frac{z^y \\exp{-z}}{y!} a(z,\\phi,p) \\exp\\{(z\\psi - k_p(\\psi))/\\phi\\} dz. \\tag{2.7} \\end{equation}\\] The integral (2.7) has no closed-form apart of the special case corresponding to the negative binomial distribution, obtained when \\(p = 2\\), i.e. a Poisson gamma mixture. In the case of \\(p=1\\), the integral (2.7) is replaced by a sum and we have the Neyman Type A distribution. Further special cases include the Hermite \\((p = 0)\\), compound Poisson \\((1 &lt; p &lt; 2)\\), factorial discrete positive stable \\((p &gt; 2)\\) and Poisson-inverse Gaussian \\((p = 3)\\) distributions (Jørgensen and Kokonendji 2015 ; C. C. Kokonendji, Dossou-Gbété, and Demétrio 2004). In spite of other approaches to compute the probability mass function of the Poisson-Tweedie distribution are available in the literature (Esnaola et al. 2013 ; Barabesi, Becatti, and Marcheselli 2016). In this material, we opted to compute it by numerical evaluation of the integral in (2.7) using the Monte Carlo method as implemented by the following functions. # Integrand Poisson X Tweedie distributions integrand &lt;- function(x, y, mu, phi, power) { int = dpois(y, lambda = x)*dtweedie(x, mu = mu, phi = phi, power = power) return(int) } # Computing the pmf using Monte Carlo dptw &lt;- function(y, mu, phi, power, control_sample) { pts &lt;- control_sample$pts norma &lt;- control_sample$norma integral &lt;- mean(integrand(pts, y = y, mu = mu, phi = phi, power = power)/norma) return(integral) } dptw &lt;- Vectorize(dptw, vectorize.args = &quot;y&quot;) When using the Monte Carlo method we need to specify a proposal distribution, from which samples will be taken to compute the integral as an expectation. In the Poisson-Tweedie case is sensible to use the Tweedie distribution as proposal. Thus, in our function we use the argument control_sample to provide these values. The advantage of this approach is that we need to simulate values once and we can reuse them for all evaluations of the probability mass function, as shown in the following code. require(tweedie) set.seed(123) pts &lt;- rtweedie(n = 1000, mu = 10, phi = 1, power = 2) norma &lt;- dtweedie(pts, mu = 10, phi = 1, power = 2) control_sample &lt;- list(&quot;pts&quot; = pts, &quot;norma&quot; = norma) dptw(y = c(0, 5, 10, 15), mu = 10, phi = 1, power = 2, control_sample = control_sample) ## [1] 0.0937 0.0590 0.0354 0.0217 dnbinom(x = c(0, 5, 10, 15), mu = 10, size = 1) ## [1] 0.0909 0.0564 0.0350 0.0218 It is also possible to use the Gauss-Laguerre method to approximate the integral in (2.7). In the supplementary material Script2.R, we provide R functions using both Monte Carlo and Gauss-Laguerre methods to approximate the probability mass function of Poisson-Tweedie distribution. Figure 2.2 presents the empirical probability mass function of some Poisson-Tweedie distributions computed based on a sample of size \\(100000\\) (gray). Furthermore, we present an approximation (black) for the probability mass function obtained by Monte Carlo integration. We considered different values of the Tweedie power parameter \\(p = 1.1\\), \\(2\\), and \\(3\\) combined with different values of the dispersion index. In all scenarios the expectation \\(\\mu\\) was fixed at \\(10\\). Figure 2.2: Empirical (gray) and approximated (black) Poisson-Tweedie probability mass function by values of the dispersion index (DI) and Tweedie power parameter. For all scenarios considered the Monte Carlo method provides a quite accurate approximation to the empirical probability mass function. For these examples, we used \\(5000\\) random samples from the proposal distribution. Finally, the Poisson-Tweedie regression model is defined by \\[Y_i \\sim PTw_{p}(\\mu_i, \\phi), \\quad \\text{with} \\quad \\mu_i = g^{-1}(\\boldsymbol{x_i}^{\\top} \\boldsymbol{\\beta}),\\] where \\(\\boldsymbol{x}_i\\) and \\(\\boldsymbol{\\beta}\\) are \\((q \\times 1)\\) vectors of known covariates and unknown regression parameters. The estimation and inference of Poisson-Tweedie regression models based on the maximum likelihood method are challenged by the presence of an intractable integral in the probability mass function and non-trivial restrictions on the power parameter space. In Chapter 3, we discuss maximum likelihood estimation for Poisson-Tweedie regression. Furthermore, in Chapter 4 we extended the Poisson-Tweedie model by using an estimating function approach in the style of Wedderburn (1974). 2.4 COM-Poisson distribution The COM-Poisson distribution belongs to the family of weighted Poisson distributions. A random variable \\(Y\\) is a weighted Poisson distribution if its probability mass function can be written in the form \\[ f(y; \\lambda, \\nu) = \\frac{\\exp^{\\{-\\lambda\\}} \\lambda^y w_y}{W y!}, \\quad y = 0, 1, \\ldots, \\] where \\(W = \\sum_{i = 0}^{\\infty} \\exp^{\\{-\\lambda\\}} \\lambda^i w_s / i!\\) is a normalizing constant (Kimberly F. Sellers, Borle, and Shmueli 2012). The COM-Poisson is obtained when \\(w_{y} = (y!)^{1-\\nu}\\) for \\(\\nu \\geq 0\\). In general, the expectation and variance of the COM-Poisson distribution cannot be expressed in closed-form. However, they can be approximated by \\[\\mathrm{E}(Y) \\approx \\lambda^{1/\\nu} - \\frac{\\nu - 1}{2 \\nu} \\quad \\text{and} \\quad \\mathrm{var}(Y) \\approx (1/ \\nu) \\lambda^{1 /\\nu}.\\] These approximations are accurate when \\(\\nu \\leq 1\\) or \\(\\lambda &gt; 10^{\\nu}\\). The infinite sum involved in computing the probability mass function of the COM-Poisson distribution can be approximated to any level of precision. It can be evaluated in R using the function dcom() from the compoisson package (Dunn 2012). Figure 2.3 presents some COM-Poisson probability mass functions. We tried to find parameters \\(\\lambda\\) and \\(\\nu\\) in order to have \\(\\mathrm{E}(Y) = 10\\) and dispersion index equals to \\(\\mathrm{DI} = 0.5, 2, 5\\) and \\(20\\). However, we could not find any parameter combination to have \\(\\mathrm{DI} = 20\\). Figure 2.3: COM-Poisson probability mass function by values of the dispersion index (DI). K. F. Sellers and Shmueli (2010) proposed a regression model based on the COM-Poisson distribution where the parameter \\(\\lambda\\) is described by the values of known covariates in a generalized linear models style. The COM-Poisson regression model is defined by \\[Y_i \\sim CP(\\lambda_i, \\nu), \\quad \\text{with} \\quad \\lambda_i = g^{-1}(\\boldsymbol{x_i}^{\\top} \\boldsymbol{\\beta}).\\] In this notation, the parameter \\(\\nu\\) is considered the dispersion parameter such that \\(\\nu &gt; 1\\) represents underdispersion and \\(\\nu &lt; 1\\) overdispersion. The Poisson model is obtained for \\(\\nu = 0\\) and as usual we adopt the logarithm link function for \\(g\\). 2.5 Comparing count distributions Let \\(Y\\) be a count random variable and \\(\\mathrm{E}(Y) = \\mu\\) and \\(\\mathrm{var}(Y)\\) denote its mean and variance, respectively. To explore and compare the flexibility of the models aforementioned, we introduce the dispersion \\((\\mathrm{DI})\\), zero-inflation \\((\\mathrm{ZI})\\) and heavy-tail \\((\\mathrm{HT})\\) indexes, which are respectively given by \\[\\begin{equation} \\mathrm{DI} = \\frac{\\mathrm{var}(Y)}{\\mathrm{E}(Y)}, \\quad \\mathrm{ZI} = 1 + \\frac{\\log \\mathrm{P}(Y = 0)}{\\mathrm{E}(Y)} \\end{equation}\\] and \\[\\begin{equation} \\mathrm{HT} = \\frac{\\mathrm{P}(Y=y+1)}{\\mathrm{P}(Y=y)}\\quad \\text{for} \\quad y \\to \\infty. \\end{equation}\\] These indexes are defined in relation to the Poisson distribution. Thus, the dispersion index indicates underdispersion for \\(\\mathrm{DI} &lt; 1\\), equidispersion for \\(\\mathrm{DI} = 1\\) and overdispersion for \\(\\mathrm{DI} &gt; 1\\). Similarly, the zero-inflated index is easily interpreted, since \\(\\mathrm{ZI} &lt; 0\\) indicates zero-deflation, \\(\\mathrm{ZI} = 0\\) corresponds to no excess of zeroes and \\(\\mathrm{ZI} &gt; 0\\) indicates zero-inflation. Finally, \\(\\mathrm{HT} \\to 1\\) when \\(y \\to \\infty\\) indicates a heavy tail distribution. For the Poisson distribution the dispersion index equals \\(1\\) \\(\\forall \\mu\\). In the Poisson case, it is easy to show that \\(\\mathrm{ZI} = 0\\) and \\(\\mathrm{HT} \\to 0\\) when \\(y \\to \\infty\\). Thus, it is quite clear that the Poisson model can deal only with equidispersed data and has no flexibility to deal with zero-inflation and/or heavy tail count data. In fact, the presented indexes were proposed in relation to the Poisson distribution in order to highlight its limitations. Figure 2.4 presents the relationship between mean and variance, the dispersion and zero-inflation indexes as a function of the expected values \\(\\mu\\) for different scenarios and count distributions. Scenario \\(1\\) corresponds to the case of underdispersion. Thus, we fixed the dispersion index at \\(DI = 0.5\\) when the mean equaling \\(10\\). Since the Poisson-Tweedie cannot deal with underdispersion, in this scenario we present only the Gamma-Count and COM-Poisson distributions. Similarly, scenarios \\(2--4\\) are obtained by fixing the dispesion index at \\(DI = 2, 5\\) and \\(10\\) when mean equaling \\(10\\). In the scenario \\(4\\) we could not find a parameter configuration in order to have a COM-Poisson distribution with dispersion index equals \\(20\\). Consequently, we present results only for the Gamma-Count and Poisson-Tweedie distributions. Furthermore, Figure 2.5 presents the heavy tail index for some extreme values of the random variable \\(Y\\). Figure 2.4: Mean and variance relationship (first line), dispersion (DI) and zero-inflation (ZI) indexes as a function of the expected values by simulation scenarios and count distributions. Figure 2.5: Heavy tail index for some extreme values of the random variable Y by simulation scenarios and count distributions. The indexes presented in Figures 2.4 and 2.5 show that for all considered scenarios the Gamma-Count and COM-Poisson distributions are quite similar. In general, for these distributions, the indexes slightly depend on the expected values and tend to stabilize for large values of \\(\\mu\\). Consequently, the mean and variance relationship is proportional to the dispersion parameter value. In the overdispersion case, the Gamma-Count and COM-Poisson distributions can handle with a limited ammount of zero-inflation and are in general light tailed distributions, i.e. \\(\\mathrm{HT} \\to 0\\) for \\(y \\to \\infty\\). Regarding the Poisson-Tweedie distributions the indexes show that for small values of the power parameter the Poisson-Tweedie distribution is suitable to deal with zero-inflated count data. In that case, the \\(\\mathrm{DI}\\) and \\(\\mathrm{ZI}\\) are almost not dependent on the values of the mean. Furthermore, the \\(\\mathrm{HT}\\) decreases as the mean increases. On the other hand, for large values of the power parameter the \\(\\mathrm{HT}\\) increases with increasing mean, showing that the model is specially suitable to deal with heavy tailed count data. In this case, the \\(\\mathrm{DI}\\) and \\(\\mathrm{ZI}\\) increase quickly as the mean increases giving an extremely overdispersed model for large values of the mean. In general, the \\(\\mathrm{DI}\\) and \\(\\mathrm{ZI}\\) are larger than one and zero, respectively, which, of course, show that the corresponding Poisson-Tweedie distributions cannot deal with underdispersed and zero-deflated count data. In terms of regression models, the Poisson and Poisson-Tweedie models are easy and convenient to interpret because the expected value is directly modelled as a function of known covariates in a generalized linear models manner. On the other hand, the Gamma-Count specifies the regression model for the expectation of the times between events and, thus requires careful interpretation. The COM-Poisson regression model is hard to interpret and compare with the traditional Poisson regression model, since it specifies the regression model for the parameter \\(\\lambda\\) that has no easy interpretation in relation to the expectation of the count response variable. Finally, in terms of computational implementation the simplicity of the Poisson regression model is unquestionable. The probability mass function of the Gamma-Count distribution requires the evaluation of the difference between two cumulative gamma distributions. For large values of the random variable \\(Y\\), such a difference can be time consuming and inaccurately computed. Similarly, the COM-Poisson probability mass function involves the evaluation of a infinity sum, which can be computational expensive and inaccurate for large values of \\(Y\\). Furthermore, for extreme values of \\(\\lambda\\) combined with small values of \\(\\nu\\) the infinite sum can numerically diverge making impossible to evaluate the probability mass function. Finally, the Poisson-Tweedie probability mass function involves an intractable integral, which makes the estimation and inference based on likelihood methods computationally intensive. "],
["likelihood.html", "Chapter 3 The method of maximum likelihood", " Chapter 3 The method of maximum likelihood The estimation and inference for the models discussed in Chapter 2 can be done by the method of maximum likelihood (Silvey 1975). In this Chapter, we present the maximum likelihood method and its main properties along with some examples in R. The maximum likelihood method is applicable mainly in situations where the true distribution of the count random variable \\(Y\\) is known apart of the values of a finite number of unknown parameters. Let \\(p(y;\\boldsymbol{\\theta})\\) denote the true probability mass function of the count random variable \\(Y\\). We assume that the family \\(p(y;\\boldsymbol{\\theta})\\) is labelled by a (\\(p \\times 1\\)) parameter vector \\(\\boldsymbol{\\theta}\\) taking values in \\(\\Theta\\) a subset of \\(\\mathbb{R}^n\\). For a given observed value \\(y\\) of the a random variable \\(Y\\), the likelihood function corresponding to the observation \\(y\\) is defined as \\(L(\\boldsymbol{\\theta};y) = p(y;\\boldsymbol{\\theta})\\). It is important to highlight that \\(p(y;\\boldsymbol{\\theta})\\) is a probability mass function on the sample space. On the other hand, \\(L(\\boldsymbol{\\theta};y) = p(y;\\boldsymbol{\\theta})\\) is a function on the parameter space \\(\\Theta\\). The likelihood function expresses the plausibilities of different parameters after we have observed \\(y\\), in the absence of any other information that we may have about these different values. In particular, for count random variables the likelihood function is the probability of the point \\(y\\) when \\(\\boldsymbol{\\theta}\\) is the true parameter. The method of maximum likelihood has a strong intuitive appeal and according to it, we estimate the true parameter \\(\\boldsymbol{\\theta}\\) by any parameter which maximizes the likelihood function. In general, there is a unique maximizing parameter which is the most plausible and this is the maximum likelihood estimate (Silvey 1975). In other words, a maximum likelihood estimate \\(\\hat{\\boldsymbol{\\theta}}(y)\\) is any element of \\(\\Theta\\) such that \\(L(\\hat{\\boldsymbol{\\theta}}(y);y) = \\underset{\\boldsymbol{\\theta}\\in \\Theta}\\max L(\\boldsymbol{\\theta};y).\\) At this stage, we make the distinction between the estimate \\(\\hat{\\boldsymbol{\\theta}}(y)\\) and the estimator \\(\\hat{\\boldsymbol{\\theta}}\\). However, we are not maintain this distinction and we shall use only \\(\\hat{\\boldsymbol{\\theta}}\\) leaving the context to make it clear whether we are thinking of \\(\\hat{\\boldsymbol{\\theta}}\\) as a function or as a particular value of a function. Let \\(Y_i\\) be independent and identically distributed count random variables with probability mass function \\(p(y;\\boldsymbol{\\theta})\\), whose observed values are denoted by \\(y_i\\) for \\(i = 1, \\ldots, n.\\) In this case, the likelihood function can be written as the product of the individuals probability mass distributions, i.e. \\[\\begin{equation} L(\\boldsymbol{\\theta};\\boldsymbol{y}) = \\prod_{i=1}^n L(\\boldsymbol{\\theta}; y_i) = \\prod_{i=1}^n p(y_i; \\boldsymbol{\\theta}). \\tag{3.1} \\end{equation}\\] For convenience, in practical situations is advisable to work with the log-likelihood function obtained by taking the logarithm of Eq. (3.1). Thus, the maximum likelihood estimator (MLE) for the parameter vector \\(\\boldsymbol{\\theta}\\) is obtained by maximizing the following log-likelihood function, \\[\\begin{equation} \\ell(\\boldsymbol{\\theta})=\\sum^n_{i=1} \\log\\{ L(\\boldsymbol{\\theta}; y_i) \\}. \\tag{3.2} \\end{equation}\\] Often, it is not possible to find a relatively simple expression in closed form for the maximum likelihood estimates. However, it is usually possible to assume that maximum likelihood estimates emerge as a solution of the likelihood equations or also called score functions, i.e. \\[\\begin{equation} \\mathcal{U}(\\boldsymbol{\\theta}) = \\left ( \\frac{\\partial \\ell(\\boldsymbol{\\theta})}{\\partial \\theta_1}^\\top, \\ldots, \\frac{\\partial \\ell(\\boldsymbol{\\theta})}{\\partial \\theta_p}^\\top \\right )^\\top = \\boldsymbol{0}. \\tag{3.3} \\end{equation}\\] However, the system of non-linear equations in (3.3) often have to be solved numerically. The entry \\((i,j)\\) of the \\(p \\times p\\) Fisher information matrix \\(\\mathcal{F}_{\\boldsymbol{\\theta}}\\) for the vector of parameter \\(\\boldsymbol{\\theta}\\) is given by \\[\\begin{equation} \\mathcal{F}_{\\boldsymbol{\\theta}_{ij}} =-\\mathrm{E} \\left \\{ \\frac{\\partial^2 \\ell(\\boldsymbol{\\theta})}{\\partial\\theta_i\\partial\\theta_j} \\right \\}. \\end{equation}\\] In order to solve the system of equations \\(\\mathcal{U}(\\boldsymbol{\\theta}) = \\boldsymbol{0}\\), we employ the Newton scoring algorithm, defined by \\[\\begin{eqnarray} \\boldsymbol{\\theta}^{(i+1)} &amp;=&amp; \\boldsymbol{\\theta}^{(i)} - \\mathcal{F}_{\\boldsymbol{\\theta}}^{-1} \\mathcal{U}(\\boldsymbol{\\theta}^{(i)}). \\end{eqnarray}\\] Finally, the well known distribution of the maximum likelihood estimator \\(\\boldsymbol{\\hat{\\theta}}\\) is \\(\\mathrm{N}(\\boldsymbol{\\theta}, \\mathcal{F}_{\\boldsymbol{\\theta}}^{-1})\\). Thus, the maximum likelihood estimator is asymptotically consistent, unbiased and efficient. A critical point of the approach described so far, is that we should be able to compute the first and second derivatives of the log-likelihood function. However, for the Gamma-Count where the log-likelihood function is given by the difference between two integrals, we cannot obtain such derivatives analytically. Similarly, for the COM-Poisson the log-likelihood function involves an infinite sum and consequently such derivatives cannot be obtained analitycally. Finally, in the Poisson-Tweedie distribution the log-likelihood function is defined by an intractable integral, which implies that we cannot obtain a closed-form for the score function and Fisher information matrix. Thus, an alternative approach is to maximize directly the log-likelihood function in equation (3.2) using a derivative-free algorithm as the Nelder-Mead method (Nelder and Mead 1965) or some other numerical method for maximizing the log-likelihood function, examples include the \\(BFGS\\), conjugate gradient and simulated annealing. All of them are implemented in R through the optim() function. The package bbmle (???) offers a suite of functions to work with numerical maximization of log-likelihood functions in R. As an example, consider the Gamma-Count distribution described in subsection 2.2. The log-likelihood function for the parameters \\(\\theta = (\\gamma, \\alpha)\\) in R is given by ll_gc &lt;- function(gamma, alpha, y) { ll &lt;- sum(dgc(y = y, gamma = gamma, alpha = alpha, log = TRUE)) return(-ll) } Thus, for a given vector of observed count values, we can numerically maximize the log-likelihood function above using the function mle2() from the bbmlepackage. It is important to highlight that by default the mle2() function requires the negative of the log-likelihood function instead of the log-likelihood itself. Thus, our function returns the negative value of the log-likelihood function. require(bbmle) y &lt;- rpois(100, lambda = 10) fit_gc &lt;- mle2(ll_gc, start = list(&quot;gamma&quot; = 10, &quot;alpha&quot; = 1), data = list(&quot;y&quot; = y)) The great advantage of the bbmle package for maximum likelihood estimation in R, is that it already provides standard methods, such as summary(), coef(), confint(), vcov(), profile() and other for objects of mle2 class. summary(fit_gc) ## Maximum likelihood estimation ## ## Call: ## mle2(minuslogl = ll_gc, start = list(gamma = 10, alpha = 1), ## data = list(y = y)) ## ## Coefficients: ## Estimate Std. Error z value Pr(z) ## gamma 9.842 0.335 29.38 &lt; 2e-16 *** ## alpha 0.929 0.139 6.68 2.5e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## -2 log L: 518 Similar functions can be done for the Poisson, Poisson-Tweedie and COM-Poisson distributions. In the supplementary material Script5.R, we provide some functions for maximum likelihood estimation of Poisson-Tweedie and COM-Poisson distributions. "],
["SM.html", "Chapter 4 Models specified by second-moment assumptions 4.1 Extended Poisson-Tweedie model 4.2 Estimation and Inference", " Chapter 4 Models specified by second-moment assumptions In Chapter 2, we presented four statistical models to deal with count data and in the Chapter 3 the method of maximum likelihood was introduced to estimate the model’s parameters. As discussed in Chapter 3 the method of maximum likelihood assumes that the true distribution of the count random variable \\(Y\\) is known apart of the values of a finite number of unknown parameters. In this Chapter, we shall present a different approach for model specification, estimation and inference based only on second-moment assumptions. 4.1 Extended Poisson-Tweedie model The Poisson-Tweedie distribution as presented in subsection the 2.3 provides a very flexible family of count distributions, however, such a family has two main drawbacks: it cannot deal with underdispersed count data and its probability mass function is given by an intractable integral, which implies that estimation based on the maximum likelihood method is computational demanding for practical data analysis. In spite of these issues Jørgensen and Kokonendji (2015) showed using factorial cumulant function that for \\(Y \\sim PTw_p(\\mu, \\phi)\\), \\(\\mathrm{E}(Y) = \\mu\\) and \\(\\mathrm{var}(Y) = \\mu + \\phi \\mu^p\\). This fact motivates Bonat et al. (2016) to specify a model by using only second-moment assumptions, i.e. mean and variance. Thus, consider a cross-section dataset, \\((y_i, \\boldsymbol{x}_i)\\), \\(i = 1, \\ldots, n\\), where \\(y_i\\)’s are i.i.d. realizations of \\(Y_i\\) according to an unspecified distribution, whose expectation and variance are given by \\[\\begin{align} \\mathrm{E}(Y_i) = \\mu_i = g^{-1}(\\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta}) \\nonumber \\\\ \\mathrm{var}(Y_i) = C_i = \\mu_i + \\phi \\mu_i^p, \\tag{4.1} \\end{align}\\] where as before \\(\\boldsymbol{x}_i\\) and \\(\\boldsymbol{\\beta}\\) are (\\(q \\times 1\\)) vectors of known covariates and unknown regression parameters and \\(g\\) is the logarithm link function. The regression model specified in (4.1) is parametrized by \\(\\boldsymbol{\\theta} = (\\boldsymbol{\\beta}^\\top, \\boldsymbol{\\lambda}^\\top )^\\top\\), where \\(\\boldsymbol{\\lambda} = (\\phi, p)\\). Note that, based on second-moment assumptions, the only restriction to have a proper model is that \\(\\mathrm{var}(Y_i) &gt; 0\\), thus \\[\\phi &gt; - \\mu^{(1-p)}_i,\\] which shows that at least at some extent negative values for the dispersion parameter are allowed. Consequently, the Poisson-Tweedie model can be extended to deal with underdispersed count data, however, in doing so the associated probability mass functions do not exist. However, in a regression modelling framework as discussed in this material, we are in general interested in the regression coefficient effects, thus such an issue does not imply any loss of interpretation and applicability. The formulation of the extended Poisson-Tweedie model is exactly the same of the quasi-binomial and quasi-Poisson models popular in the context of generalized linear models, see (Wedderburn 1974, Nelder and Wedderburn (1972)) for details. Furthermore, note that for \\(p = 1\\) the extended Poisson-Tweedie regression model corresponds to a reparametrization of the popular quasi-Poisson regression model. It is also important to highlight that in this case the relationship between mean and variance is proportional to the dispersion parameter \\(\\phi\\) as in the Gamma-Count and COM-Poisson distributions. Thus, we expect for \\(\\phi &lt; 0\\) and \\(p = 1\\) the extended Poisson-Tweedie model presents results in terms of regression coefficients really similar the ones from the Gamma-Count and COM-Poisson regression models. 4.2 Estimation and Inference Since the model presented in (4.1) is based only on second-moment assumptions the method of maximum likelihood cannot be employed. Bonat et al. (2016) based on ideas of Jørgensen and Knudsen (2004) and Bonat and Jørgensen (2016) proposed an estimating function approach for estimation and inference for the extended Poisson-Tweedie regression model. Bonat et al. (2016) combined the quasi-score and Pearson estimating functions for estimation of the regression and dispersion parameters respectively. Following Bonat et al. (2016) the quasi-score function for \\(\\boldsymbol{\\beta}\\) has the following form, \\[\\begin{equation*} \\psi_{\\boldsymbol{\\beta}}(\\boldsymbol{\\beta}, \\boldsymbol{\\lambda}) = \\left (\\sum_{i=1}^n \\frac{\\partial \\mu_i}{\\partial \\beta_1}C^{-1}_i(Y_i - \\mu_i), \\ldots, \\sum_{i=1}^n \\frac{\\partial \\mu_i}{\\partial \\beta_q}C^{-1}_i(Y_i - \\mu_i) \\right )^\\top, \\end{equation*}\\] where \\(\\partial \\mu_i/\\partial \\beta_j = \\mu_i x_{ij}\\) for \\(j = 1, \\ldots, q\\). The sensitivity matrix is defined as the expectation of the first derivative of the estimating function with respect to the model parameters. Thus, the entry \\((j,k)\\) of the \\(q \\times q\\) sensitivity matrix for \\(\\psi_{\\boldsymbol{\\beta}}\\) is given by \\[\\begin{equation} \\mathrm{S}_{\\boldsymbol{\\beta}_{jk}} = \\mathrm{E}\\left ( \\frac{\\partial}{\\partial \\beta_k} \\psi_{\\boldsymbol{\\beta}_j}(\\boldsymbol{\\beta}, \\boldsymbol{\\lambda}) \\right ) = -\\sum_{i=1}^n \\mu_i x_{ij} C^{-1}_i x_{ik} \\mu_i. \\tag{4.2} \\end{equation}\\] In a similar way, the variability matrix is defined as the variance of the estimating function. In particular, for the quasi-score function the entry \\((j,k)\\) of the \\(q \\times q\\) variability matrix is given by \\[\\begin{equation*} \\label{Vbeta} \\mathrm{V}_{\\boldsymbol{\\beta}_{jk}} = \\mathrm{Cov}(\\psi_{\\boldsymbol{\\beta}_j}(\\boldsymbol{\\beta}, \\boldsymbol{\\lambda}),\\psi_{\\boldsymbol{\\beta}_k}(\\boldsymbol{\\beta}, \\boldsymbol{\\lambda})) = \\sum_{i=1}^n \\mu_i x_{ij} C^{-1}_i x_{ik} \\mu_i. \\end{equation*}\\] The Pearson estimating function for the dispersion parameters has the following form, \\[\\begin{equation*} \\label{Pearson} \\psi_{\\boldsymbol{\\lambda}}(\\boldsymbol{\\lambda}, \\boldsymbol{\\beta}) = \\left (-\\sum_{i=1}^n \\frac{\\partial C^{-1}_i}{\\partial \\phi} \\left [ (Y_i - \\mu_i)^2 - C_i \\right ], -\\sum_{i=1}^n \\frac{\\partial C^{-1}_i}{\\partial p} \\left [ (Y_i - \\mu_i)^2 - C_i \\right ] \\right )^\\top. \\end{equation*}\\] Note that, the Pearson estimating functions are unbiased estimating functions for \\(\\boldsymbol{\\lambda}\\) based on the squared residuals \\((Y_i - \\mu_i)^2\\) with expected value \\(C_i\\). The entry \\((j,k)\\) of the \\(2 \\times 2\\) sensitivity matrix for the dispersion parameters is given by \\[\\begin{equation} \\mathrm{S}_{\\boldsymbol{\\lambda}_{jk}} = \\mathrm{E}\\left ( \\frac{\\partial}{\\partial \\lambda_k}\\psi_{\\boldsymbol{\\lambda}_j}(\\boldsymbol{\\lambda}, \\boldsymbol{\\beta}) \\right ) = -\\sum_{i=1}^n \\frac{\\partial C^{-1}_i}{\\partial \\lambda_j} C_i \\frac{\\partial C^{-1}_i}{\\partial \\lambda_k}C_i, \\tag{4.3} \\end{equation}\\] where \\(\\lambda_1\\) and \\(\\lambda_2\\) denote either \\(\\phi\\) or \\(p\\). Similarly, the cross entries of the sensitivity matrix are given by \\[\\begin{equation} \\mathrm{S}_{\\boldsymbol{\\beta}_j \\boldsymbol{\\lambda}_k} = \\mathrm{E}\\left ( \\frac{\\partial}{\\partial \\lambda_k}\\psi_{\\boldsymbol{\\beta}_j}(\\boldsymbol{\\beta}, \\boldsymbol{\\lambda}) \\right ) = 0 \\tag{4.4} \\end{equation}\\] and \\[\\begin{equation} \\mathrm{S}_{\\boldsymbol{\\lambda}_j \\boldsymbol{\\beta}_k} = \\mathrm{E}\\left ( \\frac{\\partial}{\\partial \\beta_k}\\psi_{\\boldsymbol{\\lambda}_j}(\\boldsymbol{\\lambda}, \\boldsymbol{\\beta}) \\right ) = -\\sum_{i=1}^n \\frac{\\partial C_i^{-1}}{\\partial \\lambda_j} C_i \\frac{\\partial C_i^{-1}}{\\partial \\beta_k} C_i. \\tag{4.5} \\end{equation}\\] Finally, the joint sensitivity matrix for the parameter vector \\(\\boldsymbol{\\theta}\\) is given by \\[\\begin{equation*} \\mathrm{S}_{\\boldsymbol{\\theta}} = \\begin{pmatrix} \\mathrm{S}_{\\boldsymbol{\\beta}} &amp; \\boldsymbol{0} \\\\ \\mathrm{S}_{\\boldsymbol{\\lambda}\\boldsymbol{\\beta}} &amp; \\mathrm{S}_{\\boldsymbol{\\lambda}} \\end{pmatrix}, \\end{equation*}\\] whose entries are defined by equations (4.2), (4.3), (4.4) and (4.5). We now calculate the asymptotic variance of the estimating function estimators denoted by \\(\\boldsymbol{\\hat{\\theta}}\\), as obtained from the inverse Godambe information matrix, whose general form for a vector of parameter \\(\\boldsymbol{\\theta}\\) is \\(\\mathrm{J}^{-1}_{\\boldsymbol{\\theta}} = \\mathrm{S}^{-1}_{\\boldsymbol{\\theta}} \\mathrm{V}_{\\boldsymbol{\\theta}} \\mathrm{S}^{-\\top}_{\\boldsymbol{\\theta}}\\), where \\(-\\top\\) denotes inverse transpose. The variability matrix for \\(\\boldsymbol{\\theta}\\) has the form \\[\\begin{equation} \\mathrm{V}_{\\boldsymbol{\\theta}} = \\begin{pmatrix} \\mathrm{V}_{\\boldsymbol{\\beta}} &amp; \\mathrm{V}_{\\boldsymbol{\\beta}\\boldsymbol{\\lambda}} \\\\ \\mathrm{V}_{\\boldsymbol{\\lambda}\\boldsymbol{\\beta}} &amp; \\mathrm{V}_{\\boldsymbol{\\lambda}} \\end{pmatrix}, \\tag{4.6} \\end{equation}\\] where \\(\\mathrm{V}_{\\boldsymbol{\\lambda}\\boldsymbol{\\beta}} = \\mathrm{V}^{\\top}_{\\boldsymbol{\\beta}\\boldsymbol{\\lambda}}\\) and \\(\\mathrm{V}_{\\boldsymbol{\\lambda}}\\) depend on the third and fourth moments of \\(Y_i\\), respectively. In order to avoid this dependence on higher-order moments, we propose to use the empirical versions of \\(\\mathrm{V}_{\\boldsymbol{\\lambda}}\\) and \\(\\mathrm{V}_{\\boldsymbol{\\lambda}\\boldsymbol{\\beta}}\\) as given by \\[\\begin{equation*} \\tilde{\\mathrm{V}}_{\\boldsymbol{\\lambda}_{jk}} = \\sum_{i=1}^n \\psi_{\\boldsymbol{\\lambda}_j}(\\boldsymbol{\\lambda}, \\boldsymbol{\\beta})_i\\psi_{\\boldsymbol{\\lambda}_k}(\\boldsymbol{\\lambda}, \\boldsymbol{\\beta})_i \\quad \\text{and} \\quad \\tilde{\\mathrm{V}}_{\\boldsymbol{\\lambda}_j \\boldsymbol{\\beta}_k} = \\sum_{i=1}^n \\psi_{\\boldsymbol{\\lambda}_j}(\\boldsymbol{\\lambda}, \\boldsymbol{\\beta})_i \\psi_{\\boldsymbol{\\beta}_k}(\\boldsymbol{\\lambda}, \\boldsymbol{\\beta})_i. \\end{equation*}\\] Finally, the well known asymptotic distribution of \\(\\boldsymbol{\\hat{\\theta}}\\) (Jørgensen and Knudsen 2004) is given by \\[\\begin{equation*} \\boldsymbol{\\hat{\\theta}} \\sim \\mathrm{N}(\\boldsymbol{\\theta}, \\mathrm{J}_{\\boldsymbol{\\theta}}^{-1}), \\quad \\text{where} \\quad \\mathrm{J}^{-1}_{\\boldsymbol{\\theta}} = \\mathrm{S}^{-1}_{\\boldsymbol{\\theta}} \\mathrm{V}_{\\boldsymbol{\\theta}} \\mathrm{S}^{-\\top}_{\\boldsymbol{\\theta}}. \\end{equation*}\\] To solve the system of equations \\(\\psi_{\\boldsymbol{\\beta}} = \\boldsymbol{0}\\) and \\(\\psi_{\\boldsymbol{\\lambda}} = \\boldsymbol{0}\\) Jørgensen and Knudsen (2004) proposed the modified chaser algorithm, defined by \\[\\begin{eqnarray*} \\label{chaser} \\boldsymbol{\\beta}^{(i+1)} &amp;=&amp; \\boldsymbol{\\beta}^{(i)} - \\mathrm{S}_{\\boldsymbol{\\beta}}^{-1} \\psi_{\\boldsymbol{\\beta}}(\\boldsymbol{\\beta}^{(i)}, \\boldsymbol{\\lambda}^{(i)}) \\nonumber \\\\ \\boldsymbol{\\lambda}^{(i+1)} &amp;=&amp; \\boldsymbol{\\lambda}^{(i)} - \\alpha \\mathrm{S}_{\\boldsymbol{\\lambda}}^{-1} \\psi_{\\boldsymbol{\\lambda}}(\\boldsymbol{\\beta}^{(i+1)}, \\boldsymbol{\\lambda}^{(i)}). \\end{eqnarray*}\\] The modified chaser algorithm uses the insensitivity property (4.4), which allows us to use two separate equations to update \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\lambda}\\). We introduce the tuning constant, \\(\\alpha\\), to control the step-length. This algorithm is a special case of the flexible algorithm presented by Bonat and Jørgensen (2016) in the context of multivariate covariance generalized linear models. Hence, estimation for the extended Poisson-Tweedie model is easily implemented in R through the mcglm (Bonat 2016) package. "],
["applications.html", "Chapter 5 Applications 5.1 Cotton Bolls 5.2 Soybean pod and beans 5.3 Number of vehicle claims 5.4 Radiation-induced chromosome aberration counts", " Chapter 5 Applications In this chapter, we will bring some applications based on real data sets to show how use R packages to analyse count data. 5.1 Cotton Bolls Cotton production can be drastically reduced by attack of defoliating insects. Depending on the growth stage, the plant can recover from the caused damage and keeps production not affected or can have the production reduced by low intensity defoliation. A greenhouse experiment with cotton plants (Gossypium hirsutum) was done under a completely randomized design with five replicates to assess the effects of five defoliation levels (0%, 25%, 50%,75% and 100%) on the observed number of bolls produced by plants at five growth stages: vegetative, flower-bud, blossom, fig and cotton boll. The experimental unity was a vase with two plants (Silva et al. 2012, for more). The number of cotton bolls was recorded at the hasvest of the experiment. library(lattice) library(latticeExtra) library(gridExtra) library(plyr) library(car) library(corrplot) library(doBy) library(multcomp) library(mcglm) library(MRDCr) ls(&quot;package:MRDCr&quot;) ## [1] &quot;apc&quot; &quot;calc_mean_cmp&quot; ## [3] &quot;calc_mean_gcnt&quot; &quot;calc_var_cmp&quot; ## [5] &quot;cambras&quot; &quot;capdesfo&quot; ## [7] &quot;capmosca&quot; &quot;cmp&quot; ## [9] &quot;conftemp&quot; &quot;confterm&quot; ## [11] &quot;convergencez&quot; &quot;dcmp&quot; ## [13] &quot;dgcnt&quot; &quot;dpgnz&quot; ## [15] &quot;gcnt&quot; &quot;led&quot; ## [17] &quot;llcmp&quot; &quot;llgcnt&quot; ## [19] &quot;llpgnz&quot; &quot;nematoide&quot; ## [21] &quot;ninfas&quot; &quot;panel.beeswarm&quot; ## [23] &quot;panel.cbH&quot; &quot;panel.groups.segplot&quot; ## [25] &quot;peixe&quot; &quot;pgnz&quot; ## [27] &quot;postura&quot; &quot;prepanel.cbH&quot; ## [29] &quot;seguros&quot; &quot;soja&quot; # Documentation in Portuguese. help(capdesfo, help_type = &quot;html&quot;) str(capdesfo) ## &#39;data.frame&#39;: 125 obs. of 4 variables: ## $ est : Factor w/ 5 levels &quot;vegetativo&quot;,&quot;botão floral&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ des : num 0 0 0 0 0 0.25 0.25 0.25 0.25 0.25 ... ## $ rept: int 1 2 3 4 5 1 2 3 4 5 ... ## $ ncap: int 10 9 8 8 10 11 9 10 10 10 ... levels(capdesfo$est) &lt;- c(&quot;vegetative&quot;, &quot;flower-bud&quot;, &quot;blossom&quot;, &quot;fig&quot;, &quot;cotton boll&quot;) xtabs(~est + des, data = capdesfo) ## des ## est 0 0.25 0.5 0.75 1 ## vegetative 5 5 5 5 5 ## flower-bud 5 5 5 5 5 ## blossom 5 5 5 5 5 ## fig 5 5 5 5 5 ## cotton boll 5 5 5 5 5 Figure 5.1 (top) shows the beeswarm plot of number of cotton bolls recorded for each combination of defoliation level and growth stage. All the points in the sample means and variances dispersion diagram (bottom) are below the identity line, clearly suggesting data with underdispersion. Figure 5.1: (top) Number of bolls produced for each artificial defoliation level and each growth stage. (bottom) Sample variance against the sample mean of the five replicates for each combination of defoliation level and growth stage. The exploratory data analysis, although simple, was able to detect departures from the Poisson equidispersion assumption. So, we have in advance few conditions met for the use of GLM Poisson as a regression model to analyse this experiment. Poisson, as being a process derived from the memoryless waiting times Exponential distribuition, implies that each boll is an independent event in the artificial subjacent domain, that can be thought was the natural resource domain that the plant has to allocate bolls. Its is easy to assume, based on plant fisiology, that the probability of a boll decreases with the number of previous bolls because the plant’s resource to produce bolls is limited and it is a non memoryless process equivalent. Based on the exploratory data analysis, a predictor with 2nd order effect of defoliation for each growth stage should be enough to model the number of bolls mean in a regression model. The analysis and assessment of the effects of the experimental factors are based on the Poisson, Gamma-count and Poisson Tweedie models. m0 &lt;- glm(ncap ~ est * (des + I(des^2)), data = capdesfo, family = poisson) summary(m0) ## ## Call: ## glm(formula = ncap ~ est * (des + I(des^2)), family = poisson, ## data = capdesfo) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.0771 -0.3098 -0.0228 0.2704 1.1665 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.2142 0.1394 15.89 &lt;2e-16 *** ## estflower-bud -0.0800 0.2007 -0.40 0.69 ## estblossom -0.0272 0.2001 -0.14 0.89 ## estfig -0.1486 0.2051 -0.72 0.47 ## estcotton boll 0.1129 0.1922 0.59 0.56 ## des 0.3486 0.6805 0.51 0.61 ## I(des^2) -0.7384 0.6733 -1.10 0.27 ## estflower-bud:des 0.1364 0.9644 0.14 0.89 ## estblossom:des -1.5819 1.0213 -1.55 0.12 ## estfig:des 0.4755 1.0194 0.47 0.64 ## estcotton boll:des -0.8210 0.9395 -0.87 0.38 ## estflower-bud:I(des^2) 0.1044 0.9447 0.11 0.91 ## estblossom:I(des^2) 1.4044 1.0191 1.38 0.17 ## estfig:I(des^2) -0.9294 1.0323 -0.90 0.37 ## estcotton boll:I(des^2) 1.0757 0.9210 1.17 0.24 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 75.514 on 124 degrees of freedom ## Residual deviance: 25.331 on 110 degrees of freedom ## AIC: 539.7 ## ## Number of Fisher Scoring iterations: 4 logLik(m0) ## &#39;log Lik.&#39; -255 (df=15) We fit the GLM Poisson regression model using the stardard glm() function in R. The fitted model summary shows the estimated parameters for the second order effect of defoliation crossed with growth stages levels. The residual deviance was 25.33 based on 110 degrees of freedoom. The ratio \\(25.33/110 = 0.23\\) is a strong evidence against Poisson equidispersion assumption that uses a dispersion parameter equals 1. anova(m0, test = &quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model: poisson, link: log ## ## Response: ncap ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 124 75.5 ## est 4 19.96 120 55.6 0.00051 *** ## des 1 15.86 119 39.7 6.8e-05 *** ## I(des^2) 1 1.29 118 38.4 0.25557 ## est:des 4 6.71 114 31.7 0.15212 ## est:I(des^2) 4 6.36 110 25.3 0.17388 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The analysis of deviance table did not stated effect of any interactions, neither second order effect of defiliation. Although, all these effects are noticeable in Figure ??. Figure 5.2: The 4 plots for checking departures of assumptions in the GLM-Poisson regression. Figure ?? displays the four residual plots for the fitted model. Based on these plots, there is no concern about mispecifications regarding to the model predictor or influential observations. The only remarkable aspect is about the range of the stardartized deviance residuals quite distant from the expected -3 to 3 from the normal distribution. Once more, these is another measure indicating a underdispersed count data. The gcnt() is a function defined in the MRDCr package (Walmes Marques Zeviani, Junior, and Taconeli 2016) to fit the Gamma-Count regression model. This function fits a GML-Poisson to use the estimates as initial values to optimize Gamma-Count likelihood using optim() through bblme package (Bolker and Team 2016). m1 &lt;- gcnt(ncap ~ est * (des + I(des^2)), data = capdesfo) summary(m1) ## Maximum likelihood estimation ## ## Call: ## bbmle::mle2(minuslogl = llgcnt, start = start, data = list(y = y, ## X = X, offset = off), vecpar = TRUE) ## ## Coefficients: ## Estimate Std. Error z value Pr(z) ## alpha 1.7110 0.1352 12.66 &lt; 2e-16 *** ## (Intercept) 2.2580 0.0593 38.05 &lt; 2e-16 *** ## estflower-bud -0.0765 0.0854 -0.90 0.37025 ## estblossom -0.0253 0.0851 -0.30 0.76616 ## estfig -0.1398 0.0872 -1.60 0.10885 ## estcotton boll 0.1084 0.0818 1.33 0.18476 ## des 0.3294 0.2896 1.14 0.25543 ## I(des^2) -0.6997 0.2866 -2.44 0.01464 * ## estflower-bud:des 0.1337 0.4105 0.33 0.74456 ## estblossom:des -1.5020 0.4345 -3.46 0.00055 *** ## estfig:des 0.4218 0.4336 0.97 0.33062 ## estcotton boll:des -0.7820 0.3998 -1.96 0.05046 . ## estflower-bud:I(des^2) 0.0943 0.4021 0.23 0.81452 ## estblossom:I(des^2) 1.3382 0.4335 3.09 0.00202 ** ## estfig:I(des^2) -0.8333 0.4390 -1.90 0.05768 . ## estcotton boll:I(des^2) 1.0222 0.3920 2.61 0.00911 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## -2 log L: 408 During the optimization process for this dataset, optim() has found NaN when evaluating the likelihood. This occurs due little numerical precision to calculate the difference of Gamma CDFs on tails or for extreme values, resulting in numerical zeros and corresponding -Inf log-likelihood. This is a numerical problem that can narrow, or make things difficult, the use of Gamma-Count regression model. The dispersion parameter is the first position in the parameter vector. The optimization was carried out on the log scale to avoid problems regarding to bounded parameter spaces. As the dispersion parameter is in fact interpreted as a precision coefficient, the positive estimate indicates an underdispersed count. According to the \\(z\\) statistic, \\(\\hat{alpha}\\) is significantly different from zero (Poisson case). Poisson is special case of Gamma-Count when \\(\\alpha = 0\\), so we can perform a likelihood ratio test to the hypothesis \\(H_0: \\alpha = 0\\). # Likelihood ratio test. chi &lt;- 2 * (logLik(m1) - logLik(m0)) pval &lt;- 2 * pchisq(chi, df = 1, lower.tail = FALSE) cat(&quot;Likelihood Ratio Test\\n&quot;, &quot;Chisq:\\t\\t &quot;, chi, &quot;\\n&quot;, &quot;Pr(&gt;Chisq):\\t &quot;, pval, &quot;\\n&quot;, sep = &quot;&quot;) ## Likelihood Ratio Test ## Chisq: 102 ## Pr(&gt;Chisq): 1.01e-23 # Log-likelihood profile for alpha. plot(profile(m1, which = &quot;alpha&quot;)) cbind(c(0, coef(m0)), coef(m1)) ## [,1] [,2] ## 0.0000 1.7110 ## (Intercept) 2.2142 2.2580 ## estflower-bud -0.0800 -0.0765 ## estblossom -0.0272 -0.0253 ## estfig -0.1486 -0.1398 ## estcotton boll 0.1129 0.1084 ## des 0.3486 0.3294 ## I(des^2) -0.7384 -0.6997 ## estflower-bud:des 0.1364 0.1337 ## estblossom:des -1.5819 -1.5020 ## estfig:des 0.4755 0.4218 ## estcotton boll:des -0.8210 -0.7820 ## estflower-bud:I(des^2) 0.1044 0.0943 ## estblossom:I(des^2) 1.4044 1.3382 ## estfig:I(des^2) -0.9294 -0.8333 ## estcotton boll:I(des^2) 1.0757 1.0222 rstd &lt;- summary(m1)@coef[-1, 2]/summary(m0)$coeff[, 2] plyr::each(mean, range)(rstd) ## mean range1 range2 ## 0.426 0.425 0.426 The estimates for the location parameters were very close. The ratio between Gamma-Count parameters standard error and Poisson ones, on the other hand, were 0.426 for all estimates, for 3 decimals of precision. This leads to the conclusion that TODO relação linear no parâmetro de dispersão. # Wald test for the interaction. a &lt;- c(0, attr(model.matrix(m0), &quot;assign&quot;)) ai &lt;- a == max(a) L &lt;- t(replicate(sum(ai), rbind(coef(m1) * 0), simplify = &quot;matrix&quot;)) L[, ai] &lt;- diag(sum(ai)) linearHypothesis(model = m0, # m0 is not being used here. hypothesis.matrix = L, vcov. = vcov(m1), coef. = coef(m1)) ## Linear hypothesis test ## ## Hypothesis: ## estflower - bud:I(des^2) = 0 ## estblossom:I(des^2) = 0 ## estfig:I(des^2) = 0 ## estcotton boll:I(des^2) = 0 ## ## Model 1: restricted model ## Model 2: ncap ~ est * (des + I(des^2)) ## ## Note: Coefficient covariance matrix supplied. ## ## Res.Df Df Chisq Pr(&gt;Chisq) ## 1 114 ## 2 110 4 30.5 3.8e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Fitting Poisson-Tweedie model. m2 &lt;- mcglm(linear_pred = c(ncap ~ est * (des + I(des^2))), matrix_pred = list(mc_id(data = capdesfo)), link = &quot;log&quot;, variance = &quot;poisson_tweedie&quot;, power_fixed = FALSE, data = capdesfo, control_algorithm = list(verbose = FALSE, max_iter = 100, tunning = 0.5, correct = FALSE)) ## Automatic initial values selected. # Parameter estimates. summary(m2) ## Call: ncap ~ est * (des + I(des^2)) ## ## Link function: log ## Variance function: poisson_tweedie ## Covariance function: identity ## Regression: ## Estimates Std.error Z value ## (Intercept) 2.2143 0.0627 35.308 ## estflower-bud -0.0800 0.0904 -0.886 ## estblossom -0.0265 0.0900 -0.294 ## estfig -0.1485 0.0924 -1.607 ## estcotton boll 0.1128 0.0864 1.306 ## des 0.3486 0.3065 1.137 ## I(des^2) -0.7385 0.3037 -2.432 ## estflower-bud:des 0.1361 0.4345 0.313 ## estblossom:des -1.5875 0.4612 -3.442 ## estfig:des 0.4693 0.4602 1.020 ## estcotton boll:des -0.8204 0.4228 -1.940 ## estflower-bud:I(des^2) 0.1048 0.4259 0.246 ## estblossom:I(des^2) 1.4098 0.4609 3.059 ## estfig:I(des^2) -0.9205 0.4671 -1.971 ## estcotton boll:I(des^2) 1.0752 0.4149 2.592 ## ## Power: ## Estimates Std.error Z value ## 1 1.01 0.136 7.43 ## ## Dispersion: ## Estimates Std.error Z value ## 1 -0.781 0.217 -3.59 ## ## Algorithm: chaser ## Correction: FALSE ## Number iterations: 14 plot(m2, type = &quot;algorithm&quot;) # Wald test for fixed effects. anova(m2) ## Wald test for fixed effects ## Call: ncap ~ est * (des + I(des^2)) ## ## Covariate Chi.Square Df p.value ## 1 estflower-bud 9.54 4 0.0489 ## 2 des 1.29 1 0.2555 ## 3 I(des^2) 5.91 1 0.0150 ## 4 estflower-bud:des 25.00 4 0.0001 ## 5 estflower-bud:I(des^2) 30.72 4 0.0000 # New data values for prediction. pred &lt;- with(capdesfo, expand.grid(est = levels(est), des = seq(0, 1, length.out = 30))) # Corresponding model matrix. X &lt;- model.matrix(formula(m0)[-2], data = pred) #-------------------------------------------- # Poisson. yp &lt;- predict(m0, newdata = pred, se.fit = TRUE) em &lt;- outer(yp$se.fit, c(lwrP = -1, fitP = 0, uprP = 1) * qnorm(0.975), FUN = &quot;*&quot;) ci &lt;- sweep(em, MARGIN = 1, STATS = yp$fit, FUN = &quot;+&quot;) ci &lt;- m0$family$linkinv(ci) pred &lt;- cbind(pred, as.data.frame(ci)) str(pred) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ est : Factor w/ 5 levels &quot;vegetative&quot;,&quot;flower-bud&quot;,..: 1 2 3 4 5 1 2 3 4 5 ... ## $ des : num 0 0 0 0 0 ... ## $ lwrP: num 6.97 6.37 6.72 5.88 7.91 ... ## $ fitP: num 9.15 8.45 8.91 7.89 10.25 ... ## $ uprP: num 12 11.2 11.8 10.6 13.3 ... # xyplot(fitP + lwrP + uprP ~ des | ) TODO: pares de estimativa e erros padrões. 5.2 Soybean pod and beans The tropical soils, usually poor in potassium (K), demand potassium fertilization when cultivated with soybean (Glycine max L.) to obtain satisfactory yields. Soybean production is affected by long exposition to water deficit. As postassium is a nutrient involved in the water balance in plant, by hyphotesis, a good supply of potassium avoids lose production. The aim of this study was to evaluate the effects of K doses and soil humidity levels on soybean production. The experiment was carried out in a greenhouse, in pots with two plants, containing 5 dm3 of soil. The experimental design was completely randomized block with treatments in a 5 x 3 factorial arrangement. The K doses were 0, 30, 60, 120 and 180 mg dm-3 , and the soil humidity ranged from 35 to 40, 47.5 to 52.5, and 60 to 65% of the total porosity. str(soja) ## &#39;data.frame&#39;: 75 obs. of 5 variables: ## $ K : int 0 30 60 120 180 0 30 60 120 180 ... ## $ umid: Factor w/ 3 levels &quot;37,5&quot;,&quot;50&quot;,&quot;62,5&quot;: 1 1 1 1 1 2 2 2 2 2 ... ## $ bloc: Factor w/ 5 levels &quot;I&quot;,&quot;II&quot;,&quot;III&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ ngra: int 136 159 156 171 190 140 193 200 208 237 ... ## $ nvag: int 56 62 66 68 82 63 86 94 86 97 ... # Removing an outlier. soja &lt;- soja[-74, ] soja &lt;- transform(soja, K = factor(K)) 5.2.1 Number of pods #----------------------------------------------------------------------- # Carregando e explorando os dados. xyplot(nvag + ngra ~ K, groups = umid, outer = TRUE, data = soja, type = c(&quot;p&quot;, &quot;a&quot;), scales = &quot;free&quot;, ylab = NULL, xlab = expression(&quot;Applied potassium amount&quot; ~ (mg ~ dm^{-3})), auto.key = list(title = &quot;Soil water content (%)&quot;, cex.title = 1, columns = 3), strip = strip.custom( factor.levels = c(&quot;Number of pods&quot;, &quot;Number of beans&quot;))) #-------------------------------------------- # Poisson. m0 &lt;- glm(nvag ~ bloc + umid * K, data = soja, family = poisson) ## # Checking residuals. ## par(mfrow = c(2, 2)) ## plot(m0); layout(1) #-------------------------------------------- # Gamma-Count. m1 &lt;- gcnt(formula(m0), data = soja) #-------------------------------------------- # Tweedie. m2 &lt;- mcglm(linear_pred = c(nvag ~ bloc + umid * K), matrix_pred = list(mc_id(data = soja)), link = &quot;log&quot;, variance = &quot;poisson_tweedie&quot;, power_fixed = TRUE, data = soja, control_algorithm = list(verbose = FALSE, max_iter = 100, tunning = 0.5, correct = FALSE)) ## Automatic initial values selected. #----------------------------------------------------------------------- # Comparing models. # Log-likelihood. c(P = logLik(m0), GC = logLik(m1), TW = NA) ## P GC TW ## -260 -259 NA c0 &lt;- summary(m0)$coefficients[, 1:2] c1 &lt;- summary(m1)@coef[, 1:2] c2 &lt;- rbind(summary(m2)[[1]]$tau[, 1:2], summary(m2)[[1]]$Regression[, 1:2]) # Parameter estimates according to each model. c0 &lt;- cbind(&quot;P&quot; = rbind(0, c0), &quot;GC&quot; = c1, &quot;TW&quot; = c2) colnames(c0) &lt;- substr(colnames(c0), 1, 6) round(c0, digits = 6) ## P.Esti P.Std. GC.Est GC.Std TW.Est TW.Std ## 0.0000 0.0000 0.1288 0.1655 -0.1088 0.1458 ## (Intercept) 3.9537 0.0689 3.9549 0.0646 3.9537 0.0651 ## blocII -0.0293 0.0409 -0.0293 0.0383 -0.0293 0.0386 ## blocIII -0.0727 0.0414 -0.0726 0.0388 -0.0727 0.0390 ## blocIV -0.1254 0.0419 -0.1253 0.0393 -0.1254 0.0396 ## blocV -0.1079 0.0430 -0.1079 0.0403 -0.1079 0.0406 ## umid50 0.1340 0.0877 0.1339 0.0822 0.1340 0.0827 ## umid62,5 0.2166 0.0860 0.2163 0.0806 0.2166 0.0812 ## K30 0.2743 0.0849 0.2740 0.0796 0.2743 0.0802 ## K60 0.3080 0.0843 0.3076 0.0790 0.3080 0.0796 ## K120 0.3288 0.0840 0.3285 0.0787 0.3288 0.0793 ## K180 0.2554 0.0853 0.2551 0.0799 0.2554 0.0805 ## umid50:K30 0.0632 0.1156 0.0632 0.1083 0.0632 0.1091 ## umid62,5:K30 -0.1075 0.1154 -0.1073 0.1081 -0.1075 0.1089 ## umid50:K60 0.1656 0.1137 0.1655 0.1066 0.1656 0.1073 ## umid62,5:K60 0.1074 0.1122 0.1073 0.1052 0.1074 0.1059 ## umid50:K120 0.1492 0.1134 0.1491 0.1063 0.1492 0.1070 ## umid62,5:K120 0.1184 0.1140 0.1184 0.1069 0.1184 0.1077 ## umid50:K180 0.3037 0.1136 0.3035 0.1065 0.3037 0.1072 ## umid62,5:K180 0.1984 0.1126 0.1983 0.1055 0.1984 0.1063 # Likelihhod profile for Gamma-Count dispersion parameter. plot(profile(m1, which = &quot;alpha&quot;)) abline(v = 0, lty = 2) # V &lt;- cov2cor(vcov(m1)) # corrplot.mixed(V, # lower = &quot;number&quot;, # upper = &quot;ellipse&quot;, # diag = &quot;l&quot;, # tl.pos = &quot;lt&quot;, # tl.col = &quot;black&quot;, # tl.cex = 0.8, # col = brewer.pal(9, &quot;Greys&quot;) # [-(1:3)]) dev.off() # Analysis of deviance table. anova(m0, test = &quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model: poisson, link: log ## ## Response: nvag ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 73 323 ## bloc 4 14.3 69 308 0.0064 ** ## umid 2 92.9 67 215 &lt;2e-16 *** ## K 4 136.1 63 79 &lt;2e-16 *** ## umid:K 8 14.2 55 65 0.0779 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Wald test for interaction. a &lt;- c(0, attr(model.matrix(m0), &quot;assign&quot;)) ai &lt;- a == max(a) L &lt;- t(replicate(sum(ai), rbind(coef(m1) * 0), simplify = &quot;matrix&quot;)) L[, ai] &lt;- diag(sum(ai)) linearHypothesis(model = m0, # m0 is not being used here. hypothesis.matrix = L, vcov. = vcov(m1), coef. = coef(m1)) ## Linear hypothesis test ## ## Hypothesis: ## umid50:K30 = 0 ## umid62,5:K30 = 0 ## umid50:K60 = 0 ## umid62,5:K60 = 0 ## umid50:K120 = 0 ## umid62,5:K120 = 0 ## umid50:K180 = 0 ## umid62,5:K180 = 0 ## ## Model 1: restricted model ## Model 2: nvag ~ bloc + umid * K ## ## Note: Coefficient covariance matrix supplied. ## ## Res.Df Df Chisq Pr(&gt;Chisq) ## 1 63 ## 2 55 8 16 0.043 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Wald test for fixed effects. anova(m2) ## Wald test for fixed effects ## Call: nvag ~ bloc + umid * K ## ## Covariate Chi.Square Df p.value ## 1 blocII 13.92 4 0.0076 ## 2 umid50 7.15 2 0.0280 ## 3 K30 20.93 4 0.0003 ## 4 umid50:K30 15.76 8 0.0459 5.2.2 Númber of grains #----------------------------------------------------------------------- xyplot(ngra ~ K | umid, data = soja, layout = c(NA, 1), type = c(&quot;p&quot;, &quot;smooth&quot;), ylab = &quot;Número de grãos por parcela&quot;, xlab = expression(&quot;Dose de potássio aplicada&quot;~(mg ~ dm^{3})), strip = strip.custom(strip.names = TRUE, var.name = &quot;Umidade&quot;)) soja$off &lt;- 10 fivenum(with(soja, ngra/off)) ## [1] 9.2 14.7 17.1 21.6 27.1 5.2.3 Number of grains per pod 5.3 Number of vehicle claims Em companhias de seguros é de fundamental importância especificar um preço adequado correspondente a um segurado, a fim de cobrir o risco assumido. Tal tarefa geralmente envolve a avaliação de características do plano de seguro que influenciam na taxa de sinistros observada. Nesta seção nós apresentamos a análise de um conjunto do dados referentes ao acompanhamento de 16483 clientes de uma seguradora de veículos ao longo de um ano. Os dados estão disponíveis no pacote MRDCr (com documentação em português) e podem ser carregados com ##---------------------------------------------------------------------- ## Load and organize data data(package = &quot;MRDCr&quot;) help(seguros, h = &quot;html&quot;) Após a tradução dos níveis das variáveis categóricas a estrutura dos dados fica ## Translate levels of categorical variables and colnames colnames(seguros) &lt;- c(&quot;age&quot;, &quot;sex&quot;, &quot;price&quot;, &quot;expo&quot;, &quot;nclaims&quot;) levels(seguros$sex) &lt;- c(&quot;Female&quot;, &quot;Male&quot;) str(seguros) ## &#39;data.frame&#39;: 16483 obs. of 5 variables: ## $ age : int 59 45 42 63 36 33 35 63 54 32 ... ## $ sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 2 2 1 1 2 2 2 2 2 ... ## $ price : num 24.6 23.4 86.6 77.5 25.9 ... ## $ expo : num 0.5 0.7 0.79 0.01 0.51 0.79 0.81 0.01 0.76 0.79 ... ## $ nclaims: int 1 0 0 0 0 0 0 0 0 0 ... In this dataset a variável age é mensurada em anos, e price em 1000 reais. A variável expo representa o período de cobertura do cliente, durante o ano sob análise, um valor de 0.5 significa que o cliente esteve exposto ao sinistro durante metade do ano. A Figura 5.3 mostra a descrição das variáveis a serem utilizadas na análise. Embora esses gráficos não considerem todas as variáveis conjuntamente, o gráfico à esquerda sugere que há um excesso de contagens nulas, no geral 95.183% das contagens são 0. Figure 5.3: Relative frequencies for number of vehicle claims, and empirical densities to price of vehicle, age of clients and exposure time by sex of insurance clients. Para análise desses dados consideramos efeitos quadráticos para variáveis contínuas e intercepto variando conforme sexo do cliente. price foi tomada como preditor pode ser escrito como \\[\\begin{equation*} \\begin{split} \\log\\left ( \\frac{\\mu_i}{\\texttt{expo}_i} \\right ) = &amp; \\beta_0 + \\beta_1 \\mathbb{1}(\\texttt{sex}_i) + \\beta_2 \\texttt{price}_i + \\\\ &amp; + \\beta_3 \\texttt{price}_i^2 + \\beta_4 \\texttt{age}_i + \\beta_5 \\texttt{age}_i^2 \\end{split} \\end{equation*}\\] em R definimos o preditor conforme sintaxe para objetos da classe formula. Note que a variável expo é envolta na função offset e sendo assim é considerado apenas como denominador das contagens. ## Define preditor form0 &lt;- nclaims ~ offset(log(expo)) + sex + price + I(price^2) + age + I(age^2) Nós ajustamos os modelos de regressão Poisson e Poisson-Tweedie usando os frameworks stats::glm e mcglm::mcglm, que se baseam em máxima verossimilhança e especificação por momentos respectivamente. ## Fit Poisson m0PO &lt;- glm(form0, data = seguros, family = poisson) ## Fit Poisson-Tweedie m0PT &lt;- mcglm( linear_pred = c(form0), matrix_pred = list(mc_id(seguros)), link = &quot;log&quot;, variance = &quot;poisson_tweedie&quot;, power_fixed = FALSE, data = seguros) ## Automatic initial values selected. Os parâmetros de regressão estimados nos modelos Poisson e Poisson-Tweedie são exibidos abaixo juntamente com seus erros padrão. Uma coluna com a razão entre as estimatimas e erros-padrão é acrescida. Note que as estimativas muito similares e os erros-Padrão são em torno de 20% maiores quando considerado o modelo Poisson-Tweedie. ##---------------------------------------------------------------------- ## Parameter estimates parPO &lt;- summary(m0PO)$coefficients[, 1:2] parPT &lt;- summary(m0PT)[[1]]$Regression[, 1:2] ## Call: nclaims ~ offset(log(expo)) + sex + price + I(price^2) + age + ## I(age^2) ## ## Link function: log ## Variance function: poisson_tweedie ## Covariance function: identity ## Regression: ## Estimates Std.error Z value ## (Intercept) -2.277314 4.76e-01 -4.79 ## sexMale -0.190856 7.69e-02 -2.48 ## price 0.016413 6.03e-03 2.72 ## I(price^2) -0.000115 5.65e-05 -2.03 ## age -0.032210 1.82e-02 -1.77 ## I(age^2) 0.000301 1.72e-04 1.75 ## ## Power: ## Estimates Std.error Z value ## 1 2.2 1.12 1.96 ## ## Dispersion: ## Estimates Std.error Z value ## 1 12.1 37.4 0.322 ## ## Algorithm: chaser ## Correction: TRUE ## Number iterations: 10 pars &lt;- cbind(parPO, parPT) cbind(pars, cbind(&quot;RatioEst&quot; = pars[, 3]/pars[, 1], &quot;RatioStd&quot; = pars[, 4]/pars[, 2])) ## Estimate Std. Error Estimates Std.error RatioEst ## (Intercept) -1.703449 3.91e-01 -2.277314 4.76e-01 1.337 ## sexMale -0.175449 6.38e-02 -0.190856 7.69e-02 1.088 ## price 0.018583 5.28e-03 0.016413 6.03e-03 0.883 ## I(price^2) -0.000125 5.03e-05 -0.000115 5.65e-05 0.921 ## age -0.038209 1.50e-02 -0.032210 1.82e-02 0.843 ## I(age^2) 0.000336 1.41e-04 0.000301 1.72e-04 0.895 ## RatioStd ## (Intercept) 1.22 ## sexMale 1.20 ## price 1.14 ## I(price^2) 1.12 ## age 1.22 ## I(age^2) 1.22 Embora tenhamos ajustados os modelos e avaliados seus resultados, uma suposição que é inerente ao modelo não foi avaliada. A inclusão do offset (exposição) pressupõe relação identidade entre a exposição e o número médio de sinistros (\\(\\mu_i \\texttt{expo}_i = \\lambda_i\\)), em outras palavras, sob as mesmas condições esperamos que um indivíduo com tempo de exposição de \\(1\\) ano tenha o dobro de sinistros do que um indivíduo com tempo de exposição de \\(0.5\\). A avaliação dessa suposição é realizada estimando esse coeficiente e comparando-o com o valor fixado. ## Define preditors (free offset of first order and second order) form1 &lt;- nclaims ~ log(expo) + sex + price + I(price^2) + age + I(age^2) ## Fit Poisson m1PO &lt;- glm(form1, data = seguros, family = poisson) ## Fit Poisson-Tweedie m1PT &lt;- mcglm(linear_pred = c(form1), matrix_pred = list(mc_id(seguros)), link = &quot;log&quot;, variance = &quot;poisson_tweedie&quot;, power_fixed = FALSE, data = seguros) ## Automatic initial values selected. Nos modelos Poisson o método anova em R realiza o teste de razão de verossimilhanças para modelos aninhados. ## Analysis of deviance table for nested models (an &lt;- anova(m0PO, m1PO, test = &quot;Chisq&quot;)) ## Analysis of Deviance Table ## ## Model 1: nclaims ~ offset(log(expo)) + sex + price + I(price^2) + age + ## I(age^2) ## Model 2: nclaims ~ log(expo) + sex + price + I(price^2) + age + I(age^2) ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 16477 6495 ## 2 16476 6269 1 226 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Com a estimação do coeficiente para \\(\\log(\\texttt{expo})\\), houve uma diferença de 226.5 em relação ao modelo cujo coeficiente é fixado em 1, evidenciando que a suposição de identidade entre o a exposição e as contagens não é atendida. Para os modelos Poisson-Tweedie os testes de razão de verossimilhanças também são possíveis, porém tendem a ser computacionalmente intensivos uma vez que a função de densidade é definida por uma integral intratável. Sendo assim uma alternativa é comparar os modelos via measures of Goodness-of-Fit que não precisam da verossimilhança like pseudo Gaussian log-likelihood (plogLik), pseudo Akaike Information Criterion (pAIC), pseudo Kullback-Leibler Information Criterion (pKLIC) and Error Sum of Squares (ESS), que além de mais rápidas podem ser utilizadas para o modelo Poisson-Tweedie estendido que não se baseia em verossimilhança. Essas medidas são calculadas com função mcglm::gof. ## Measures of goodness-of-fit for compare nested models goflist &lt;- lapply(list(m0PT, m1PT), gof) (gofPT &lt;- do.call(&quot;rbind&quot;, goflist)) ## plogLik Df pAIC pKLIC pBIC ## 1 -3276 8 6569 6744 6630 ## 2 -3126 9 6270 6445 6339 Da mesma forma nos modelos Poisson-Tweedie também há fortes indicações de que o coeficiente para o logarítimo das exposições não seja \\(1\\). Sendo assim seguimos as análises com os modelos que consideram a estimação do efeito dos tempos de exposição. As estimativas pontuais com erros-padrão são obtidas da mesma forma que nos primeiros modelos ajustados. Note que não houve uma mudança drástica nas estimativas comparando ao modelo com offset, indicando que não há relação entre os tempos de exposição e as covariáveis. A similaridade das estimativas do modelo Poisson e Poisson-Tweedie se mantém assim como o aumento em 20% nos erros-padrão. ##---------------------------------------------------------------------- ## Parameter estimates parPO &lt;- summary(m1PO)$coefficients[, 1:2] parPT &lt;- summary(m1PT)[[1]]$Regression[, 1:2] ## Call: nclaims ~ log(expo) + sex + price + I(price^2) + age + I(age^2) ## ## Link function: log ## Variance function: poisson_tweedie ## Covariance function: identity ## Regression: ## Estimates Std.error Z value ## (Intercept) -2.092150 4.75e-01 -4.41 ## log(expo) 0.192759 4.72e-02 4.08 ## sexMale -0.184397 7.68e-02 -2.40 ## price 0.016408 6.11e-03 2.69 ## I(price^2) -0.000114 5.75e-05 -1.98 ## age -0.034693 1.81e-02 -1.91 ## I(age^2) 0.000320 1.71e-04 1.87 ## ## Power: ## Estimates Std.error Z value ## 1 1.83 0.873 2.1 ## ## Dispersion: ## Estimates Std.error Z value ## 1 4.36 10.5 0.416 ## ## Algorithm: chaser ## Correction: TRUE ## Number iterations: 10 pars &lt;- cbind(parPO, parPT) cbind(pars, cbind(&quot;RatioEst&quot; = pars[, 3]/pars[, 1], &quot;RatioStd&quot; = pars[, 4]/pars[, 2])) ## Estimate Std. Error Estimates Std.error RatioEst ## (Intercept) -2.135916 3.91e-01 -2.092150 4.75e-01 0.980 ## log(expo) 0.186930 4.11e-02 0.192759 4.72e-02 1.031 ## sexMale -0.184568 6.38e-02 -0.184397 7.68e-02 0.999 ## price 0.017008 5.21e-03 0.016408 6.11e-03 0.965 ## I(price^2) -0.000120 4.94e-05 -0.000114 5.75e-05 0.949 ## age -0.033467 1.49e-02 -0.034693 1.81e-02 1.037 ## I(age^2) 0.000308 1.41e-04 0.000320 1.71e-04 1.039 ## RatioStd ## (Intercept) 1.21 ## log(expo) 1.15 ## sexMale 1.20 ## price 1.17 ## I(price^2) 1.16 ## age 1.21 ## I(age^2) 1.21 Finalmente a Figura @(fig:claims-pred) apresenta as curvas de predição conforme cada variável, como temos mais de uma covariável numérica no modelo, fixamos as demais variáveis seu valor mediano para construção das curvas. O efeito quadrático das variáveis price e age é evidente. A média de sinistros é maior para veículos entre 50 e 100 mil reais, veículos de valores baixos ou muito elevados tendem a ter um taxa de sinistros menor, o que faz sentido no mercado brasileiro onde furtos e acidentes ocorrem com maior frequência em carros populares. Para a idade temos a interpretação contrária, espera-se menos sinistros para idades medianas, entre 40 e 70 anos e maiores para jovens e idosos. Comparando os modelos temos curvas de predição e intervalos de confiança muito similares, sendo levemente maiores quando considerado o Poisson-Tweedie. O que se destaque no gráfico é a diferença nos intervalos de confiança para o preço do veículo, nesse caso o modelo Poisson-Tweedie é muito mais conservador onde há menos observações, no intervalo de preços mais elevados. cap &lt;- paste(&quot;Curves of predict values an confidence intervals (95\\\\%)&quot;, &quot;based on Poisson and Poisson-Tweedie regression models&quot;, &quot;for each numerical covariate setting the others in the&quot;, &quot;median.&quot;) ##------------------------------------------- ## Prediction to exposure aux &lt;- with(seguros, { expand.grid( expo = seq(min(expo), max(expo), length.out = 30), sex = unique(sex), price = median(price), age = median(age) ) }) da &lt;- data.frame(var = &quot;Exposure&quot;, x = aux$expo, sex = aux$sex) X &lt;- model.matrix(update(form1, NULL ~ .), data = aux) pred &lt;- list(PO = da, PT = da) ## Poisson model aux &lt;- confint(glht(m1PO, linfct = X), calpha = univariate_calpha())$confint colnames(aux)[1] &lt;- &quot;fit&quot; pred$PO &lt;- cbind(pred$PO, exp(aux)[, c(&quot;lwr&quot;, &quot;fit&quot;, &quot;upr&quot;)]) ## Poisson-Tweedie model qn &lt;- qnorm(0.975) * c(lwr = -1, fit = 0, upr = 1) V &lt;- vcov(m1PT) i &lt;- grepl(&quot;beta&quot;, colnames(V)) eta &lt;- X %*% coef(m1PT, type = &quot;beta&quot;)$Estimates std &lt;- sqrt(diag(as.matrix(X %*% as.matrix(V[i, i]) %*% t(X)))) me &lt;- outer(std, qn, FUN = &quot;*&quot;) aux &lt;- sweep(me, 1, eta, FUN = &quot;+&quot;) pred$PT &lt;- cbind(pred$PT, exp(aux)) ## Organize predictions predsex &lt;- ldply(pred, .id = &quot;model&quot;) ##------------------------------------------- ## Prediction to price aux &lt;- with(seguros, { expand.grid( expo = median(expo), sex = unique(sex), price = seq(min(price), max(price), length.out = 30), age = median(age) ) }) da &lt;- data.frame(var = &quot;Price&quot;, x = aux$price, sex = aux$sex) X &lt;- model.matrix(update(form1, NULL ~ .), data = aux) pred &lt;- list(PO = da, PT = da) ## Poisson model aux &lt;- confint(glht(m1PO, linfct = X), calpha = univariate_calpha())$confint colnames(aux)[1] &lt;- &quot;fit&quot; pred$PO &lt;- cbind(pred$PO, exp(aux)[, c(&quot;lwr&quot;, &quot;fit&quot;, &quot;upr&quot;)]) ## Poisson-Tweedie model qn &lt;- qnorm(0.975) * c(lwr = -1, fit = 0, upr = 1) V &lt;- vcov(m1PT) i &lt;- grepl(&quot;beta&quot;, colnames(V)) eta &lt;- X %*% coef(m1PT, type = &quot;beta&quot;)$Estimates std &lt;- sqrt(diag(as.matrix(X %*% as.matrix(V[i, i]) %*% t(X)))) me &lt;- outer(std, qn, FUN = &quot;*&quot;) aux &lt;- sweep(me, 1, eta, FUN = &quot;+&quot;) pred$PT &lt;- cbind(pred$PT, exp(aux)) ## Organize predictions predspr &lt;- ldply(pred, .id = &quot;model&quot;) ##------------------------------------------- ## Prediction to age aux &lt;- with(seguros, { expand.grid( expo = median(expo), sex = unique(sex), price = median(price), age = seq(min(age), max(age), length.out = 30) ) }) da &lt;- data.frame(var = &quot;Age&quot;, x = aux$age, sex = aux$sex) X &lt;- model.matrix(update(form1, NULL ~ .), data = aux) pred &lt;- list(PO = da, PT = da) ## Poisson model aux &lt;- confint(glht(m1PO, linfct = X), calpha = univariate_calpha())$confint colnames(aux)[1] &lt;- &quot;fit&quot; pred$PO &lt;- cbind(pred$PO, exp(aux)[, c(&quot;lwr&quot;, &quot;fit&quot;, &quot;upr&quot;)]) ## Poisson-Tweedie model qn &lt;- qnorm(0.975) * c(lwr = -1, fit = 0, upr = 1) V &lt;- vcov(m1PT) i &lt;- grepl(&quot;beta&quot;, colnames(V)) eta &lt;- X %*% coef(m1PT, type = &quot;beta&quot;)$Estimates std &lt;- sqrt(diag(as.matrix(X %*% as.matrix(V[i, i]) %*% t(X)))) me &lt;- outer(std, qn, FUN = &quot;*&quot;) aux &lt;- sweep(me, 1, eta, FUN = &quot;+&quot;) pred$PT &lt;- cbind(pred$PT, exp(aux)) ## Organize predictions predsag &lt;- ldply(pred, .id = &quot;model&quot;) ##------------------------------------------- ## Graph preds &lt;- rbind(predsex, predspr, predsag) useOuterStrips( xyplot(fit ~ x | var + sex, data = preds, groups = model, type = c(&quot;g&quot;, &quot;l&quot;), ly = preds$lwr, uy = preds$upr, layout = c(NA, 1), as.table = TRUE, alpha = 0.2, xlab = &quot;Value of covariate&quot;, ylab = &quot;Mean of vehicles claims&quot;, ## scales = list(x = &quot;free&quot;), scales = &quot;free&quot;, auto.key = list( columns = 2, lines = TRUE, points = FALSE, text = c(&quot;Poisson&quot;, &quot;Poisson-Tweedie&quot;) ), cty = &quot;bands&quot;, fill = &quot;gray80&quot;, panel = panel.superpose, panel.groups = panel.cbH, prepanel = prepanel.cbH) ) Para avaliar o poder preditivo dos modelos nós calculamos as frequências estimadas pelos dois modelos considerados e comparamos com as observadas. As frequências estimadas \\(\\hat{Fr}\\), para um dado valor \\(y\\) são calculadas como \\[ \\hat{Fr}(y) = \\sum_{i=1}^n \\Pr(Y=y \\mid \\boldsymbol{\\hat{\\theta}}_i) \\] Onde \\(\\Pr(Y=y \\mid \\hat{\\boldsymbol{\\theta}}_i)\\) é a função massa de probabilidade definida pelo conjunto de parâmetros \\(\\boldsymbol{\\hat{\\theta}}_i\\). Para o modelo Poisson \\(\\hat{\\boldsymbol{\\theta}}_i = \\hat{\\mu}_i\\) e para o modelo Poisson-Tweedie \\(\\hat{\\boldsymbol{\\theta}}_i = [\\hat{\\mu}_i \\hat{\\phi}=4.359] \\hat{p}=1.832]\\). For Poisson-Tweedie model we evaluate the integral using the Gauss-Laguerre method (with 100 points). The results shows that Poisson-Tweedie model offers a better fit with adjust frequencies very close of observed frequencies. ## Adjust frequencies by models ## Calcule probabilities X &lt;- model.matrix(form1, data = seguros) y &lt;- 0:6 n &lt;- nrow(seguros) freqs &lt;- list() ## Observed freqs$Obs &lt;- with(seguros, sapply(y, function(x) { sum(nclaims == x) })) ## By Poisson muPO &lt;- exp(X %*% coef(m1PO)) probsPO &lt;- do.call( &quot;rbind&quot;, lapply(muPO, function(mui) { py &lt;- dpois(y, lambda = mui) })) freqs$PO &lt;- round(apply(probsPO, 2, sum)) ## By Poisson-Tweedie ## -- very time consuming. muPT &lt;- exp(X %*% coef(m1PT, type = &quot;beta&quot;)$Estimates) phi &lt;- with(coef(m1PT), Estimates[Type == &quot;tau&quot;]) power &lt;- with(coef(m1PT), Estimates[Type == &quot;power&quot;]) probsPT &lt;-do.call( &quot;rbind&quot;, lapply(muPT, function(mui) { py &lt;- dptweedie(y = y, mu = mui, phi = phi, power = power, n_pts = 100, method = &quot;laguerre&quot;) })) freqs$PT &lt;- round(apply(probsPT, 2, sum)) tabf &lt;- ldply(freqs) colnames(tabf) &lt;- c(&quot;&quot;, y) tabf ## 0 1 2 3 4 5 6 ## 1 Obs 15689 602 166 22 3 1 0 ## 2 PO 15498 953 31 1 0 0 0 ## 3 PT 15673 663 121 26 6 2 0 5.4 Radiation-induced chromosome aberration counts ## The density of PTW dsitributions (using numerical integration) library(statmod) library(tweedie) integrand &lt;- function(x, y, mu, phi, power) { int = dpois(y, lambda = x)* dtweedie(x, mu = mu, phi = phi, power = power) return(int) } # Numerical integration using Gauss-Laguerre method gauss_laguerre &lt;- function(integrand, n_pts, y, mu, phi, power) { pts &lt;- gauss.quad(n_pts, kind=&quot;laguerre&quot;) integral &lt;- sum(pts$weights*integrand(pts$nodes, y = y, mu = mu, phi = phi, power = power)/ exp(-pts$nodes)) return(integral) } gauss_laguerre_vec &lt;- Vectorize(FUN = gauss_laguerre, vectorize.args = &quot;y&quot;) # Numerical integration using Monte Carlo method ----------------------- monte_carlo &lt;- function(integrand, n_pts, y, mu, phi, power) { pts &lt;- rtweedie(n_pts, mu = mu, phi = phi, power = power) norma &lt;- dtweedie(pts, mu = mu, phi = phi, power = power) integral &lt;- mean(integrand(pts, y = y, mu = mu, phi = phi, power = power)/norma) return(integral) } # Probability mass function Poisson-Tweedie ---------------------------- dptweedie_aux &lt;- function(y, mu, phi, power, n_pts, method) { if(method == &quot;laguerre&quot; | y &gt; 0) { pmf &lt;- gauss_laguerre(integrand = integrand, n_pts = n_pts, y = y, mu = mu, phi = phi, power = power) } if(method == &quot;laguerre&quot; &amp; y == 0) { v.y &lt;- round(10*sqrt(mu + phi*mu^power),0) if(v.y &gt; 1000) {v.y &lt;- 1000} #print(v.y) y.grid &lt;- 0:v.y temp &lt;- gauss_laguerre_vec(integrand = integrand, n_pts = n_pts, y = y.grid, mu = mu, phi = phi, power = power) pmf &lt;- 1-sum(temp)+temp[1] } if(method == &quot;MC&quot;) { pmf &lt;- monte_carlo(integrand = integrand, n_pts = n_pts, y = y, mu = mu, phi = phi, power = power) } return(pmf) } # Vectorize version dptweedie &lt;- Vectorize(FUN = dptweedie_aux, vectorize.args = c(&quot;y&quot;,&quot;mu&quot;)) In biological dosimetry studies essentially the response variables are data counts. These experiments measure the number of chromosome aberrations in human lymphocytes after to controlled exposure of ionizing radiation. The aim of the studies are analyse the biological effects induced by ionizing radiation. The aberrations most commonly mensured are the dicentrics, centric rings, and micronucle . In this section the dataset considered was obtained after irradiating blood samples with five different doses between 0.1 and 1 Gy of 2.1 MeV neutrons. In this case, the frequencies of dicentrics and centric rings after a culture of 72 hours are analysed. The dataset was analysed by (???), as an example of zero-inflated data and Bonat et al. (2016), using extend Poisson-Tweedie approach. The data are available in data/chromossome.rda file. In R it can be loaded with ##---------------------------------------------------------------------- ## Load load(&quot;./data/chromosome.rda&quot;) str(chromosome) ## &#39;data.frame&#39;: 5232 obs. of 2 variables: ## $ ndic: num 0 0 0 0 0 0 0 0 0 0 ... ## $ dose: num 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ... The Figure 5.4 shows the frequencies chromosome aberrations (left) and average of chromosome aberrations by radiation doses (right). Note that the highest frequencies are observed for zero counts and the averages are between 0 and 1, suggesting excess zero. Figure 5.4: Observed frequencies of the chromosome aberrations counts by radiation doses (left) and means of chromosome aberrations for each radiation doses (right). ##---------------------------------------------------------------------- ## Modelling form &lt;- ndic ~ dose + I(dose^2) m0PO &lt;- glm(form, family = poisson, data = chromosome) m0GC &lt;- gcnt(form, data = chromosome) m0CP &lt;- cmp(form, data = chromosome, sumto = 100) m0PT &lt;- mcglm( linear_pred = c(form), matrix_pred = list(mc_id(chromosome)), link = &quot;log&quot;, variance = &quot;poisson_tweedie&quot;, power_fixed = FALSE, data = chromosome, control_algorithm = list( verbose = FALSE, max_iter = 1000, tunning = 0.5, tol = 1e-10) ) ## Automatic initial values selected. ##---------------------------------------------------------------------- ## Goodness of fit ## Compute logLik for Poisson-Tweedie ## ** especific for this example logLik.mcglm &lt;- function(object) { y &lt;- c(0:5, 7) data &lt;- data.frame(dose = unique(chromosome$dose)) ## --- form &lt;- update(object$linear_pred[[1]], NULL ~ .) X &lt;- model.matrix(form, data) mu &lt;- exp(X %*% coef(object, type = &quot;beta&quot;)$Estimates) phi &lt;- with(coef(object), Estimates[Type == &quot;tau&quot;]) power &lt;- with(coef(object), Estimates[Type == &quot;power&quot;]) obs &lt;- xtabs(~ndic + dose, data = chromosome) matpred &lt;- do.call(&quot;cbind&quot;, lapply(mu, function(mui) { dptweedie(y = y, mu = mui, phi = phi, power = power, n_pts = 180, &quot;laguerre&quot;) })) ll &lt;- sum(log(matpred) * obs) attr(ll, &quot;df&quot;) &lt;- nrow(coef(object)) attr(ll, &quot;nobs&quot;) &lt;- m0PT$n_obs class(ll) &lt;- &quot;logLik&quot; return(ll) } ## Compute table of gof models &lt;- list(&quot;Poisson&quot; = m0PO, &quot;Gamma-Count&quot; = m0GC, &quot;COM-Poisson&quot; = m0CP, &quot;POisson-Tweedie&quot; = m0PT) (measures &lt;- sapply(models, function(x) c(&quot;LogLik&quot; = logLik(x), &quot;AIC&quot; = AIC(x), &quot;BIC&quot; = BIC(x)))) ## Poisson Gamma-Count COM-Poisson POisson-Tweedie ## LogLik -2995 -2966 -2967 -2951 ## AIC 5997 5940 5943 5911 ## BIC 6016 5966 5969 5944 cap &lt;- paste(&quot;Dispersion diagram of observed chromossome aberrations and&quot;, &quot;curves of predict values an confidence intervals (95\\\\%)&quot;, &quot;based on Poisson, Gamma-Count, COM-Poisson and&quot;, &quot;Poisson-Tweedie regression models.&quot;) ##------------------------------------------- ## Visualize means ## pred2 &lt;- subset(preds, model %in% c(&quot;PO&quot;, &quot;PT&quot;)) xyplot(fit ~ dose, data = preds, groups = model, type = c(&quot;g&quot;, &quot;l&quot;), ly = preds$lwr, uy = preds$upr, layout = c(NA, 1), as.table = TRUE, alpha = 0.2, xlab = &quot;Radiation doses&quot;, ylab = &quot;Mean of chromosomic aberrations&quot;, auto.key = list( columns = 2, lines = TRUE, points = FALSE, text = c(&quot;Poisson&quot;, &quot;Gamma-Count&quot;, &quot;COM-Poisson&quot;, &quot;Poisson-Tweedie&quot;) ), cty = &quot;bands&quot;, fill = &quot;gray80&quot;, panel = panel.superpose, panel.groups = panel.cbH, prepanel = prepanel.cbH) + as.layer( update(xy2, type = &quot;p&quot;) ) Figure 5.5: Dispersion diagram of observed chromossome aberrations and curves of predict values an confidence intervals (95%) based on Poisson, Gamma-Count, COM-Poisson and Poisson-Tweedie regression models. ##------------------------------------------- ## Calcule probabilities index &lt;- preds$dose %in% unique(chromosome$dose) means &lt;- preds[index, c(&quot;model&quot;, &quot;dose&quot;, &quot;fit&quot;)] probs &lt;- list() y &lt;- 0:5 ## Poisson model indPO &lt;- grep(&quot;PO&quot;, means$model) probs$PO &lt;- do.call( &quot;rbind&quot;, lapply(indPO, function(i) { with(means, { py &lt;- dpois(y, fit[i]) data.frame(dose = dose[i], y = y, prob = py) }) }) ) ## Gamma-Count model indGC &lt;- grep(&quot;GC&quot;, means$model) alpha &lt;- exp(coef(m0GC)[&quot;alpha&quot;]) probs$GC &lt;- do.call( &quot;rbind&quot;, lapply(indGC, function(i) { with(means, { aux &lt;- predict(m0GC, newdata = t(cbind(X[i, ])), type = &quot;link&quot;) lambda &lt;- alpha %*% exp(aux) py &lt;- dgcnt(y, lambda = lambda, alpha = alpha) data.frame(dose = dose[i], y = y, prob = py) }) }) ) ## COM-Poisson model indCP &lt;- grep(&quot;CP&quot;, means$model) nu &lt;- exp(coef(m0CP)[&quot;phi&quot;]) sumto &lt;- m0CP@data$sumto probs$CP &lt;- do.call( &quot;rbind&quot;, lapply(indCP, function(i) { with(means, { aux &lt;- predict(m0CP, newdata = t(cbind(X[i, ])), type = &quot;link&quot;) lambda &lt;- exp(aux) py &lt;- dcmp(y, lambda = lambda, nu = nu, sumto = sumto) data.frame(dose = dose[i], y = y, prob = py) }) }) ) ## Poisson-Tweedie model indPT &lt;- grep(&quot;PT&quot;, means$model) phi &lt;- with(coef(m0PT), Estimates[Type == &quot;tau&quot;]) power &lt;- with(coef(m0PT), Estimates[Type == &quot;power&quot;]) probs$PT &lt;- do.call( &quot;rbind&quot;, lapply(indPT, function(i) { with(means, { py &lt;- dptweedie(y = y, mu = fit[i], phi = phi, power = power, n_pts = 180, method = &quot;laguerre&quot;) data.frame(dose = dose[i], y = y, prob = py) }) }) ) ## Visualize the probabilities daprobs &lt;- ldply(probs, .id = &quot;model&quot;) barchart(prob ~ y | factor(dose), groups = model, data = daprobs, horizontal = FALSE, axis = axis.grid, as.table = TRUE, origin = 0, xlab = &quot;Number of chromosomic aberrations&quot;, ylab = &quot;Probability&quot;, scales = list(x = list(labels = y)), auto.key = list( columns = 2, text = c(&quot;Poisson&quot;, &quot;Gamma-Count&quot;, &quot;COM-Poisson&quot;, &quot;Poisson-Tweedie&quot;) ), strip = strip.custom( strip.name = TRUE, var.name = &quot;dose&quot;, sep = &quot; = &quot; )) "],
["bibliography.html", "Bibliography", " Bibliography "]
]
