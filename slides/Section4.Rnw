<<setup-childSection4, include=FALSE>>=
## set_parent("slides-mrdcr.Rnw")

## Pacotes utilizados nesta seção

@

\subsection{Especificação}
%%%-------------------------------------------------------------------
\begin{frame}{Motivação}
\begin{itemize}
  \item Assumem que a distribuição de probabilidade é completamente
  conhecida a menos de um vetor de parâmetros.
  \item Na prática pode ser difícil escolher uma particular distribuição.
  \item Difícil de estimar usando métodos baseados em verossimilhança.
  \item Nem sempre a esperança é conhecida.
  \item O efeito das covariáveis não são diretamente relacionados a esperança da va.
  \item Abordagem mais geral que se adapte automaticamente a estrutura de dispersão dos dados.
  \item Fácil de implementar.
  \item SOLUÇÃO: Poisson-Tweedie estendida.
\end{itemize}
\end{frame}

%%%-------------------------------------------------------------------
\begin{frame}[c]
\frametitle{Modelos de regressão}
\begin{itemize}
\item Considere $(y_i, \boldsymbol{x}_i)$, $i = 1, \ldots, n$, onde
$y_i$'s são iid. va's.
\item Especificação paramétrica completa: $$Y_i \sim \mathrm{PTw_p}(\mu_i, \phi).$$
\item Especificação baseada em momentos:
\begin{eqnarray}
\label{marginalGaussian}
\mathrm{E}(Y_i) &=& \mu_i \nonumber    \\
\mathrm{var}(Y_i) &=& \mu_i + \phi\mu_i^p \nonumber
\end{eqnarray}
onde $g(\mu_i) = \eta_i = \boldsymbol{x}_i^{\top} \boldsymbol{\beta}$,
$\boldsymbol{x}_i$ e $\boldsymbol{\beta}$ são ($p \times 1$) vetores
de covariáveis conhecidas e parâmetros de regressão desconhecidos.
\item $\mathrm{var}(Y_i) > 0$, assim $\phi > - \mu^{(1-p)}_i$ $\Longrightarrow$ sub, equi e superdispersão.
\item $g$ função de ligação (log link).
\end{itemize}
\end{frame}
%=======================================================================

%=======================================================================
\begin{frame}[c]
\frametitle{Modelos de regressão}
\begin{itemize}
\item Poisson-Tweedie estendida pode lidar com subdispersão $\phi < 0$.
\begin{figure}[h]
\includegraphics[scale=0.4]{images/Rplot.pdf}
\caption{\tiny{Índice de dispersão como uma função da média por valores dos parâmetros de dispersão e potência.}}
\label{SUBPTW}
\centering
\end{figure}
\item Máxima verossimilhança precisa da especificação paramétrica completa.
\item Funções de estimação (Bonat, et. al. 2016)
\item Espaço paramétrico para o parâmetro de potência é livre ($p \in \Re$).
\end{itemize}
\end{frame}
%=======================================================================

%=======================================================================
\subsection{Estimação e Inferência}
\begin{frame}[c]
\frametitle{Parâmetros de regressão}
\begin{itemize}
\item Seja $\boldsymbol{\theta} = (\boldsymbol{\beta}^\top, \boldsymbol{\lambda}^\top = (\phi, p)^\top)^\top$ 
o vetor de parâmetros.
\item Função quasi-score para os parâmetros de regressão
\begin{equation*}
\psi_{\boldsymbol{\beta}}(\boldsymbol{\beta}, \boldsymbol{\lambda}) = \left (\sum_{i=1}^n \frac{\partial \mu_i}{\partial \beta_1}C^{-1}_i(y_i - \mu_i)^\top, \ldots, \sum_{i=1}^n \frac{\partial \mu_i}{\partial \beta_Q}C^{-1}_i(y_i - \mu_i)^\top  \right )^\top,
\end{equation*}
onde $C_i = \mu_i + \phi \mu_i^p$ e $\partial \mu_i/\partial \beta_j = \mu_i x_{ij}$ para $j = 1, \ldots, p$.
\item A entrada $(j,k)$ da matriz $p \times p$ de sensitividade para $\psi_{\boldsymbol{\beta}}$ é dada por
\begin{equation}
\label{Sbeta}
\mathrm{S}_{\beta_{jk}} = \mathrm{E}\left ( \frac{\partial}{\partial \beta_k} \psi_{\beta_j}(\boldsymbol{\beta}, \boldsymbol{\lambda})  \right ) = -\sum_{i=1}^n \mu_i x_{ij} C^{-1}_i x_{ik} \mu_i.
\end{equation}
\item A enrada $(j,k)$ da matriz $p \times p$  de variabilidade para $\psi_{\boldsymbol{\beta}}$ é dada por
\begin{equation}
\label{Vbeta}
\mathrm{V}_{\beta_{jk}} = \mathrm{Var}(\psi_{\beta_{jk}}(\boldsymbol{\beta}, \boldsymbol{\lambda})) = \sum_{i=1}^n \mu_i x_{ij} C^{-1}_i x_{ik} \mu_i.
\end{equation}
\end{itemize}
\end{frame}
%=======================================================================

%=======================================================================
\begin{frame}[c]
\frametitle{Parâmetros de dispersão}
\begin{itemize}
\item Função de estimação de Pearson
\begin{equation*}
\label{Pearson}
\psi_{\boldsymbol{\lambda}}(\boldsymbol{\lambda}, \boldsymbol{\beta}) = \left (\sum_{i=1}^n W_{i \phi} \left [ (y_i - \mu_i)^2 - C_i \right ]^\top, \sum_{i=1}^n W_{i p} \left [ (y_i - \mu_i)^2 - C_i \right ]^\top  \right )^\top,
\end{equation*}
onde $W_{i \phi} = - \partial C^{-1}_i/\partial \phi$ e $W_{i p} = - \partial C^{-1}_i/\partial p$.
\item A entrada $(j,k)$ data matriz $2 \times 2$ de sensitividade é dada por 
\begin{equation}
\label{Slambda}
\mathrm{S}_{\boldsymbol{\lambda}_{jk}} = \mathrm{E}\left ( \frac{\partial}{\partial \lambda_k}\psi_{\lambda_j}(\boldsymbol{\lambda}, \boldsymbol{\beta})  \right ) = -\sum_{i=1}^n W_{i \lambda_j} C_i W_{i\lambda_k}C_i, 
\end{equation}
onde $\lambda_1$ e $\lambda_2$ denota ambos $\phi$ ou $p$.
\end{itemize}
\end{frame}
%=======================================================================

%=======================================================================
\begin{frame}[c]
\frametitle{Matriz de sensitividade cruzada}
\begin{itemize}
\item A matriz de sensitividade cruzada é dada por
\begin{equation}
\label{Sbetalambda}
\mathrm{S}_{\beta_j \lambda_k} = \mathrm{E}\left ( \frac{\partial}{\partial \lambda_k}\psi_{\beta_j}(\boldsymbol{\beta}, \boldsymbol{\lambda})  \right ) = 0
\end{equation}
e
\begin{equation}
\label{Slambdabeta}
\mathrm{S}_{\lambda_j \beta_k} = \mathrm{E}\left ( \frac{\partial}{\partial \beta_k}\psi_{\lambda_j}(\boldsymbol{\lambda}, \boldsymbol{\beta})  \right ) = -\sum_{i=1}^n W_{i\lambda_j}C_i W_{i\beta_k}C_i,
\end{equation}
onde $W_{i\beta_k} = -\partial C_i^{-1}/\partial \beta_k$. 
\item A matriz de sensitividade conjunta para o vetor $\boldsymbol{\theta}$ é dada por
\begin{equation*}
\mathrm{S}_{\boldsymbol{\theta}} = \begin{pmatrix}
\mathrm{S}_{\boldsymbol{\beta}} & \boldsymbol{0} \\ 
\mathrm{S}_{\boldsymbol{\lambda}\boldsymbol{\beta}} & \mathrm{S}_{\boldsymbol{\lambda}}
\end{pmatrix},
\end{equation*}
cuja entradas são definidas por (\ref{Sbeta}), (\ref{Slambda}), (\ref{Sbetalambda}) e (\ref{Slambdabeta}).
\end{itemize}
\end{frame}
%=======================================================================

%=======================================================================
\begin{frame}[c]
\frametitle{Matriz de variabilidade}
\begin{itemize}
\item A matriz de variabilidade para $\boldsymbol{\theta}$ tem a forma
\begin{equation*}
\label{VTHETA}
\mathrm{V}_{\boldsymbol{\theta}} = \begin{pmatrix}
\mathrm{V}_{\boldsymbol{\beta}} & \mathrm{V}^{\top}_{\boldsymbol{\lambda}\boldsymbol{\beta}} \\ 
\mathrm{V}_{\boldsymbol{\lambda}\boldsymbol{\beta}} & \mathrm{V}_{\boldsymbol{\lambda}}
\end{pmatrix}
\end{equation*}
\item $\mathrm{V}_{\boldsymbol{\beta}}$ foi definido em~(\ref{Vbeta}).
\item As entradas para matriz de variabilidade empírica são dadas por
\begin{eqnarray*}
\tilde{\mathrm{V}}_{\lambda_{jk}} = \sum_{i=1}^n \psi_{\lambda_j}(\boldsymbol{\lambda}, \boldsymbol{\beta})_i \psi_{\lambda_k}(\boldsymbol{\lambda}, \boldsymbol{\beta})_i \quad \text{and} \\  \tilde{\mathrm{V}}_{\lambda_j \beta_k} = \sum_{i=1}^n \psi_{\lambda_j}(\boldsymbol{\lambda}, \boldsymbol{\beta})_i \psi_{\beta_k}(\boldsymbol{\lambda}, \boldsymbol{\beta})_i.
\end{eqnarray*}
\end{itemize}
\end{frame}
%=======================================================================

%=======================================================================
\begin{frame}[c]
\frametitle{Distribuição assintótica e algoritmo de ajuste}
\begin{itemize}
\item Faça $\boldsymbol{\hat{\theta}}$ denotar o estimador função de estimação.
\item A distribuição assintótica de $\boldsymbol{\hat{\theta}}$ é dada por 
\begin{equation*}
\boldsymbol{\hat{\theta}} \sim \mathrm{N}(\boldsymbol{\theta}, \mathrm{J}_{\boldsymbol{\theta}}^{-1}), \quad
\text{onde} \quad
\mathrm{J}_{\boldsymbol{\theta}} =  \mathrm{S}_{\boldsymbol{\theta}}^{-1}\mathrm{V}_{\boldsymbol{\theta}}\mathrm{S}_{\boldsymbol{\theta}}^{-1}
\end{equation*}
é a matriz de informação de Godambe.
\item Algoritmo Chaser
\begin{eqnarray*}
\label{chaser}
\boldsymbol{\beta}^{(i+1)} &=& \boldsymbol{\beta}^{(i)} - \mathrm{S}_{\boldsymbol{\beta}}^{-1} \psi_{\boldsymbol{\beta}}(\boldsymbol{\beta}^{(i)}, \boldsymbol{\lambda}^{(i)}) \nonumber \\
\boldsymbol{\lambda}^{(i+1)} &=& \boldsymbol{\lambda}^{(i)} - \alpha \mathrm{S}_{\boldsymbol{\lambda}}^{-1} \psi_{\boldsymbol{\lambda}}(\boldsymbol{\beta}^{(i+1)}, \boldsymbol{\lambda}^{(i)}).
\end{eqnarray*}
\item Facilmente implementado em \texttt{R} através da função \texttt{mcglm()}
do pacote \texttt{mcglm}(Bonat, 2015).
\item $\alpha$ é um \textit{tuning} constante para controlar o tamanho do passo.
\end{itemize}
\end{frame}
%=======================================================================
